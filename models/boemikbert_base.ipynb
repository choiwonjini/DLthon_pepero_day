{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"NGgUL6s0M18o"}},{"cell_type":"markdown","metadata":{"id":"B8uoccF-xdVF"},"source":["# **ì‚¬ìš©ë²•**\n","í˜„ì¬ ì´ íŒŒì¼ì€ ê¹ƒí—ˆë¸Œì—ì„œ ë°”ë¡œ í•„ìš”í™”ì¼ì„ êµ¬ê¸€ ì½”ë©ì— ë¡œë“œí•˜ê³  êµ¬ê¸€ ë“œë¼ì´ë¸Œ(ìë£Œ ë° ì €ì¥ê³µê°„)ì„ ì´ìš©í•´ì„œ ì‘ë™í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŒ. ë”°ë¼ì„œ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ ì•ë¶€ë¶„ì„ ì¡°ì •í•˜ê³  íŒŒì¼ íŒ¨ìŠ¤ë¥¼ ì§€ì •í•˜ë©´ ë¨.\n","\n","ê·¸ë¦¬ê³  ê²°ê³¼ê°’ì„ ìš”ì•½í•˜ëŠ” Optuna ë³´ê³ ì„œë„ ìë™ ìƒì„±ë˜ë‹ˆ í™•ì¸í•˜ì…”ì„œ íŒŒì¼ ê²½ë¡œë¥¼ í•„ìš”ì— ë”°ë¼ ì§€ì •í•˜ì„¸ìš”\n","\n","ì˜ˆì¸¡ ì•„ì›ƒí’‹ì€ predition.csví™”ì¼ëª…ìœ¼ë¡œ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ë‚˜ì˜¤ê²Œ í˜„ì¬ëŠ” ë˜ì–´ ìˆìœ¼ë‹ˆ ê²½ë¡œ,ì´ë¦„, í¬ë§·(í˜„ì¬ëŠ” 1ê°œ ë ˆì´ë¸”(ì‹ ë¢°ë„)ì´ ì¶”ê°€ ë˜ì–´ìˆìŒë‹¤!)ì„ ìˆ˜ì • ë°˜ì˜í•˜ë©´ ë  ë“¯í•´ìš”. ì´í›„ ë¸”ëŸ­ì— ì•„ì›ƒí’‹ì„ ë¶„ì„í•˜ëŠ” ë³„ë„ ì½”ë“œ ë¸”ëŸ­ë“¤ì´ ë˜ì–´ ìˆì–´ìš”. ì•„ì›ƒí’‹ì„ ë°”ë¡œ í™•ì¸í•˜ì‹œê¸¸!\n","  \n","ì´í›„ ë³„ë„ ì¶”ê°€ ë¸”ëŸ­ì— ì•„ì›ƒí’‹ì„ ì¼€ê¸€ í¬ë§·ì— ë§ì¶”ì–´ ìë™ìœ¼ë¡œ submission ìš”êµ¬í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” íŒŒì¼ë„ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë‹ˆ íŒ¨ìŠ¤ë¥¼ ì§€ì •í•´ì„œ í™œìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤."]},{"cell_type":"markdown","source":["# **ì†Œê°œ ë° ìš”ì•½**\n","\n","BERT Modelì€ transformerëª¨ë¸ì— pre-trained ëª¨ë¸ë¡œ boemi-kcbert baseë¥¼ í™œìš©. Boemi-kcbertëŠ” naverì˜ ëŒ“ê¸€ì„ ë² ì´ìŠ¤ë¡œ í•´ì„œ í•™ìŠµëœ ëª¨ë¸ë¡œ ë¹„ì†ì–´, ë¶€ì •ì–´ í•œê¸€ ë¶„ë¥˜ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  í‰ê°€ë¨. í•´ë‹¹ ëª¨ë¸ë¡œ ì‹¤í–‰ ê²°ê³¼ ì¼ë°˜ ëŒ€í™” ë¶„ë¥˜ ì„±ê³¼ê°€ ì¦ê°€í•œ ê²ƒì´ ê´€ì°°ë¨. íŒ€ì˜ ì„ íƒê°€ëŠ¥í•œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¼ê³  ìƒê°ë¨.\n","\n","í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” optunaë¥¼ í™œìš©, ìë™ìµœì í™”ë¥¼ ë„ëª¨í–ˆìŒ. optuna ë°©ì‹ì€ ë¨¼ì € trialë¡œ í•™ìŠµì„ ì§„í–‰í•´ì„œ ê·¸ì¤‘ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ ì •í•´ì„œ ìµœì¢… í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì•½ê°„ ë” ì¶”ê°€ì ì¸ ì‹œê°„ì†Œìš”ê°€ ë°œìƒí•¨. ê²°ë¡ ì ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½ë§Œìœ¼ë¡œëŠ” ë¯¸ë¯¸í•œ ê°œì„ íš¨ê³¼ë¥¼ ë³´ì˜€ê³  í•µì‹¬ì ì¸ ê°œì„  ì˜ì—­ì€ ì•„ë‹ˆë¼ê³  íŒë‹¨ë¨.\n","\n","ë²„íŠ¸ ëª¨ë¸ì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•´ì„œ í´ë¡œë“œë¥¼ í†µí•´ì„œ ì „ì²´ë¥¼ êµ¬ì„±í•˜ê³ , geminië¥¼ í†µí•´ì„œ ìˆ˜ì •í–ˆìŒ. ëª¨ë¸ ê°œë°œì„ ìš°ì„ ì ìœ¼ë¡œ ì™„ì„±í•˜ê¸° ìœ„í•˜ì—¬, ëª¨ë“ˆí™”ëœ íŒ€ì˜ ì½”ë“œëŠ” ì‚¬ìš©í•˜ì§€ ëª»í–ˆìœ¼ë¯€ë¡œ ê¹Šì€ ì–‘í•´ë¥¼ ë¶€íƒí•©ë‹ˆë‹¤. ì²˜ìŒë¶€í„° êµ¬ì„±í•œ ìƒíƒœì„."],"metadata":{"id":"MwzBwPySig2F"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32595,"status":"ok","timestamp":1762796369662,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"},"user_tz":-540},"id":"lSZu-4gxGIHu","outputId":"ca60d23f-4729-45f0-ff37-b30dc8a0fba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["GitHub token ì…ë ¥: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","Cloning into 'DLthon_pepero_day'...\n","remote: Enumerating objects: 309, done.\u001b[K\n","remote: Counting objects: 100% (21/21), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 309 (delta 7), reused 4 (delta 4), pack-reused 288 (from 2)\u001b[K\n","Receiving objects: 100% (309/309), 128.66 MiB | 20.71 MiB/s, done.\n","Resolving deltas: 100% (134/134), done.\n"]}],"source":["# ê¹ƒí—ˆë¸Œì—ì„œ ì½”ë©ìœ¼ë¡œ ìë£Œë¥¼ ë°”ë¡œ ê°€ì ¸ì˜¤ë„ë¡ ì…‹íŒ…. ê° í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • ìš”ë§\n","from getpass import getpass\n","token = getpass('GitHub token ì…ë ¥: ')\n","!git clone https://{token}@github.com/hiyslee/DLthon_pepero_day.git"]},{"cell_type":"code","source":["#êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ì €ì¥ì†Œë¡œ ì„ íƒí•¨\n","# ============================================================================\n","# 01. Google Drive ë§ˆìš´íŠ¸\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"01. Google Drive ë§ˆìš´íŠ¸\")\n","print(\"=\"*70)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"âœ“ Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZ6KbOrfYdWU","executionInfo":{"status":"ok","timestamp":1762796391240,"user_tz":-540,"elapsed":19975,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"}},"outputId":"bfe16d09-ca95-4ebf-d73e-d58ed5068bfc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","01. Google Drive ë§ˆìš´íŠ¸\n","======================================================================\n","Mounted at /content/drive\n","âœ“ Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\n","\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5449,"status":"ok","timestamp":1762796401402,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"},"user_tz":-540},"id":"1e972834","outputId":"b2aaf52b-4464-4367-e06a-8aefade6970a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n","Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.3\n","/bin/bash: line 1: !pip: command not found\n"]}],"source":["# íŠ¸ëœìŠ¤í¬ë¨¸ ì„¤ì¹˜. ì—ëŸ¬ ìƒê²¨ì„œ ê°œì„ í•¨\n","!pip install --upgrade pip && !pip install torch && !pip install transformers --upgrade"]},{"cell_type":"code","source":["# beomi/kcbert-base ëª¨ë¸ - ì™„ë²½í•œ ì „ì²˜ë¦¬ + í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ + ì‹ ë¢°ë„ ë¶„ë¥˜\n","\"\"\"\n","ì§ì¥ ë‚´ ëŒ€í™” ë¶„ë¥˜ - BERT ëª¨ë¸ (ì™„ì „íŒ)\n","Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ + ì™„ë²½í•œ ì „ì²˜ë¦¬ + í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n","\n","âœ¨ ì£¼ìš” ê¸°ëŠ¥:\n","  â€¢ ì™„ë²½í•œ ë°ì´í„° ì „ì²˜ë¦¬ (BOM ì œê±°, ê³µë°± ì •ê·œí™”, ê¸¸ì´ í•„í„°ë§ ë“±)\n","  â€¢ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚° ë° ì ìš© (ë¶ˆê· í˜• í•´ê²°)\n","  â€¢ Optunaë¥¼ ì´ìš©í•œ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n","  â€¢ ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ ì¡°ì • (3ë‹¨ê³„)\n","    - High (â‰¥0.85): ë†’ì€ ì‹ ë¢°ë„ - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","    - Mid (0.6-0.85): ì¤‘ê°„ ì‹ ë¢°ë„ - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","    - Low (<0.6): ë‚®ì€ ì‹ ë¢°ë„ - \"ì¼ë°˜ ëŒ€í™”\"ë¡œ ì¬ë¶„ë¥˜\n","  â€¢ Top-K ì˜ˆì¸¡ ì œê³µ\n","  â€¢ ì‹ ë¢°ë„ ë“±ê¸‰ë³„ ì¶œë ¥ íŒŒì¼ ìƒì„±\n","  â€¢ ìƒì„¸í•œ í†µê³„ ë¶„ì„ ë° ì‹œê°í™”\n","\n","ğŸ”¥ ê°œì„  ì‚¬í•­:\n","  - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜ ì ìš©)\n","  - ì‹ ë¢°ë„ ì„ê³„ê°’ ìµœì í™” (0.8 â†’ 0.6)\n","  - í…ìŠ¤íŠ¸ í’ˆì§ˆ í–¥ìƒ (ì™„ë²½í•œ ì „ì²˜ë¦¬)\n","  - ë°ì´í„° ê²€ì¦ ê°•í™”\n","\"\"\"\n","\n","# ============================================================================\n","# 01. í™˜ê²½ ì„¤ì • - GPU ë©”ëª¨ë¦¬ ìµœì í™”\n","# ============================================================================\n","import os\n","\n","# âœ… PyTorch ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","# âœ… Transformers ìºì‹œ ë¹„í™œì„±í™” (ì„ íƒì‚¬í•­)\n","os.environ['TRANSFORMERS_OFFLINE'] = '0'\n","\n","print(\"=\"*70)\n","print(\"01. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# ============================================================================\n","# 02. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"02. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\")\n","print(\"=\"*70)\n","\n","import subprocess\n","import sys\n","\n","packages = ['torch', 'transformers', 'scikit-learn', 'tqdm', 'pandas', 'numpy', 'optuna', 'matplotlib']\n","for package in packages:\n","    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n","from torch.optim import AdamW\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","import optuna\n","from optuna.trial import TrialState\n","import time\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import matplotlib\n","matplotlib.use('Agg')\n","import re\n","\n","print(\"âœ“ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 03. ê¸°ë³¸ ì„¤ì •ê°’\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"03. ê¸°ë³¸ ì„¤ì •\")\n","print(\"=\"*70)\n","\n","OUTPUT_PATH = '/content/drive/My Drive/predictions.csv'\n","TUNING_REPORT_PATH = '/content/drive/My Drive/optuna_tuning_report.txt'\n","TRAIN_PATH = '/content/DLthon_pepero_day/Data/aiffel-dl-thon-dktc-online-15/train.csv'\n","TEST_PATH = '/content/DLthon_pepero_day/Data/aiffel-dl-thon-dktc-online-15/test.csv'\n","\n","MODEL_NAME = 'beomi/kcbert-base'\n","VALIDATION_RATIO = 0.2\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Optuna ì„¤ì •\n","N_TRIALS = 5\n","TIMEOUT_PER_TRIAL = 1800\n","\n","# ğŸ¯ ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ ì„¤ì • (3ë‹¨ê³„) - ìµœì í™”ë¨\n","CONFIDENCE_THRESHOLD_MID = 0.6   # Mid/Low ê²½ê³„ (0.8 â†’ 0.6ìœ¼ë¡œ í•˜í–¥)\n","CONFIDENCE_THRESHOLD_HIGH = 0.85  # High/Mid ê²½ê³„ (0.9 â†’ 0.85ë¡œ í•˜í–¥)\n","TOP_K_PREDICTIONS = 3\n","USE_TOP_K = True\n","\n","# ì „ì²˜ë¦¬ ì„¤ì •\n","MIN_TEXT_LENGTH = 10     # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´\n","MAX_TEXT_LENGTH = 1500   # ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (í† í° ì œí•œ ê³ ë ¤)\n","\n","print(f\"âœ“ ëª¨ë¸: {MODEL_NAME}\")\n","print(f\"âœ“ ë””ë°”ì´ìŠ¤: {DEVICE}\")\n","print(f\"âœ“ Optuna ì‹œë„ íšŸìˆ˜: {N_TRIALS}\")\n","print(f\"âœ“ Validation ë¹„ìœ¨: {VALIDATION_RATIO*100}%\")\n","print(f\"\\nâœ“ ì‹ ë¢°ë„ ì„ê³„ê°’ (3ë‹¨ê³„) - ìµœì í™”:\")\n","print(f\"  â€¢ High: â‰¥ {CONFIDENCE_THRESHOLD_HIGH}\")\n","print(f\"  â€¢ Mid: {CONFIDENCE_THRESHOLD_MID} ~ {CONFIDENCE_THRESHOLD_HIGH}\")\n","print(f\"  â€¢ Low: < {CONFIDENCE_THRESHOLD_MID} â†’ 'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜\")\n","print(f\"\\nâœ“ ì „ì²˜ë¦¬ ì„¤ì •:\")\n","print(f\"  â€¢ ìµœì†Œ ê¸¸ì´: {MIN_TEXT_LENGTH}ì\")\n","print(f\"  â€¢ ìµœëŒ€ ê¸¸ì´: {MAX_TEXT_LENGTH}ì\\n\")\n","\n","# âœ… GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥\n","if torch.cuda.is_available():\n","    print(f\"ğŸ’¾ GPU ì •ë³´:\")\n","    print(f\"  â€¢ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"  â€¢ ì´ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n","    print()\n","\n","# ============================================================================\n","# 04. ì™„ë²½í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"04. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\")\n","print(\"=\"*70)\n","\n","def remove_bom(text):\n","    \"\"\"BOM (Byte Order Mark) ì œê±°\"\"\"\n","    if isinstance(text, str):\n","        return text.replace('\\ufeff', '').replace('\\ufffe', '')\n","    return text\n","\n","def normalize_whitespace(text):\n","    \"\"\"ê³µë°±, íƒ­, ì¤„ë°”ê¿ˆ ì •ê·œí™”\"\"\"\n","    if not isinstance(text, str):\n","        return text\n","\n","    # íƒ­ì„ ê³µë°±ìœ¼ë¡œ\n","    text = text.replace('\\t', ' ')\n","\n","    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n","    text = re.sub(r' +', ' ', text)\n","\n","    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ 2ê°œë¡œ (ëŒ€í™” êµ¬ì¡° ìœ ì§€)\n","    text = re.sub(r'\\n\\n+', '\\n\\n', text)\n","\n","    # ì•ë’¤ ê³µë°± ì œê±°\n","    text = text.strip()\n","\n","    return text\n","\n","def normalize_quotes(text):\n","    \"\"\"ë”°ì˜´í‘œ ì •ê·œí™”\"\"\"\n","    if not isinstance(text, str):\n","        return text\n","\n","    # ë‹¤ì–‘í•œ ë”°ì˜´í‘œë¥¼ í‘œì¤€ ë”°ì˜´í‘œë¡œ\n","    quote_map = {\n","        '\"': '\"', '\"': '\"',  # í°ë”°ì˜´í‘œ\n","        ''': \"'\", ''': \"'\",  # ì‘ì€ë”°ì˜´í‘œ\n","        'â€³': '\"',  # ì¸ì¹˜ ê¸°í˜¸\n","        'â€¶': '\"',  # ì—­ ì¸ì¹˜ ê¸°í˜¸\n","    }\n","\n","    for old, new in quote_map.items():\n","        text = text.replace(old, new)\n","\n","    return text\n","\n","def clean_special_chars(text):\n","    \"\"\"ì œì–´ ë¬¸ì ì œê±°\"\"\"\n","    if not isinstance(text, str):\n","        return text\n","\n","    # ì œì–´ ë¬¸ì ì œê±° (ì¤„ë°”ê¿ˆ, íƒ­ ì œì™¸)\n","    text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\t')\n","\n","    return text\n","\n","def clean_text(text):\n","    \"\"\"ì „ì²´ í…ìŠ¤íŠ¸ ì •ì œ íŒŒì´í”„ë¼ì¸\"\"\"\n","    text = remove_bom(text)\n","    text = normalize_whitespace(text)\n","    text = normalize_quotes(text)\n","    text = clean_special_chars(text)\n","    return text\n","\n","def is_valid_conversation(text, min_length=10, max_length=2000):\n","    \"\"\"ëŒ€í™” ìœ íš¨ì„± ê²€ì¦\"\"\"\n","    if not isinstance(text, str):\n","        return False\n","\n","    # ë¹ˆ ë¬¸ìì—´\n","    if not text or not text.strip():\n","        return False\n","\n","    # ê¸¸ì´ ì²´í¬\n","    if len(text) < min_length or len(text) > max_length:\n","        return False\n","\n","    # ì˜ë¯¸ ìˆëŠ” ë¬¸ìê°€ ìˆëŠ”ì§€\n","    if not re.search(r'[ê°€-í£a-zA-Z0-9]', text):\n","        return False\n","\n","    return True\n","\n","def preprocess_dataframe(df, min_length=10, max_length=1500, verbose=True):\n","    \"\"\"ì™„ë²½í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n","\n","    if verbose:\n","        print(f\"\\nğŸ§¹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n","        print(f\"  ì›ë³¸ ë°ì´í„°: {len(df):,}ê°œ\")\n","\n","    initial_count = len(df)\n","    stats = {'original': initial_count, 'steps': []}\n","\n","    # 1. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n","    required_columns = ['conversation', 'class']\n","    if not all(col in df.columns for col in required_columns):\n","        raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {required_columns}\")\n","\n","    # 2. ê²°ì¸¡ì¹˜ ì œê±°\n","    before = len(df)\n","    df = df.dropna(subset=['conversation', 'class'])\n","    removed = before - len(df)\n","    if verbose and removed > 0:\n","        print(f\"  âœ“ ê²°ì¸¡ì¹˜ ì œê±°: {removed}ê°œ\")\n","    stats['steps'].append(('ê²°ì¸¡ì¹˜', removed))\n","\n","    # 3. ë¹ˆ ë¬¸ìì—´ ì œê±°\n","    before = len(df)\n","    df = df[df['conversation'].str.strip().str.len() > 0]\n","    removed = before - len(df)\n","    if verbose and removed > 0:\n","        print(f\"  âœ“ ë¹ˆ ë¬¸ìì—´ ì œê±°: {removed}ê°œ\")\n","    stats['steps'].append(('ë¹ˆ ë¬¸ìì—´', removed))\n","\n","    # 4. í…ìŠ¤íŠ¸ ì •ì œ\n","    if verbose:\n","        print(f\"  âœ“ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘ (BOM, ê³µë°±, ë”°ì˜´í‘œ, íŠ¹ìˆ˜ë¬¸ì)...\")\n","    df['conversation'] = df['conversation'].apply(clean_text)\n","    df['class'] = df['class'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n","\n","    # 5. ê¸¸ì´ í•„í„°ë§\n","    before = len(df)\n","    df['_temp_length'] = df['conversation'].str.len()\n","\n","    too_short = (df['_temp_length'] < min_length).sum()\n","    too_long = (df['_temp_length'] > max_length).sum()\n","\n","    df = df[(df['_temp_length'] >= min_length) & (df['_temp_length'] <= max_length)]\n","    df = df.drop('_temp_length', axis=1)\n","\n","    removed = before - len(df)\n","    if verbose and removed > 0:\n","        print(f\"  âœ“ ê¸¸ì´ í•„í„°ë§: {removed}ê°œ ì œê±° (ì§§ìŒ: {too_short}, ê¹€: {too_long})\")\n","    stats['steps'].append(('ê¸¸ì´ í•„í„°ë§', removed))\n","\n","    # 6. ìœ íš¨ì„± ê²€ì¦\n","    before = len(df)\n","    df = df[df['conversation'].apply(lambda x: is_valid_conversation(x, min_length, max_length))]\n","    removed = before - len(df)\n","    if verbose and removed > 0:\n","        print(f\"  âœ“ ìœ íš¨ì„± ê²€ì¦: {removed}ê°œ ì œê±°\")\n","    stats['steps'].append(('ìœ íš¨ì„±', removed))\n","\n","    # 7. ì¤‘ë³µ ì œê±°\n","    before = len(df)\n","    df = df.drop_duplicates(subset=['conversation', 'class'])\n","    removed = before - len(df)\n","    if verbose and removed > 0:\n","        print(f\"  âœ“ ì¤‘ë³µ ì œê±°: {removed}ê°œ\")\n","    stats['steps'].append(('ì¤‘ë³µ', removed))\n","\n","    # 8. ì¸ë±ìŠ¤ ì¬ì„¤ì •\n","    df = df.reset_index(drop=True)\n","\n","    stats['final'] = len(df)\n","    stats['total_removed'] = initial_count - len(df)\n","    stats['retention_rate'] = (len(df) / initial_count * 100) if initial_count > 0 else 0\n","\n","    if verbose:\n","        print(f\"\\n  âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n","        print(f\"  ìµœì¢… ë°ì´í„°: {len(df):,}ê°œ (ìœ ì§€ìœ¨: {stats['retention_rate']:.1f}%)\")\n","        total_removed = sum(count for _, count in stats['steps'])\n","        print(f\"  ì´ ì œê±°: {total_removed}ê°œ\\n\")\n","\n","    return df, stats\n","\n","print(\"âœ“ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 05. Dataset í´ë˜ìŠ¤ ì •ì˜\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"05. Dataset í´ë˜ìŠ¤ ì •ì˜\")\n","print(\"=\"*70)\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, conversations, tokenizer, labels=None, max_length=512):\n","        self.conversations = conversations\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.conversations)\n","\n","    def __getitem__(self, idx):\n","        text = self.conversations[idx]\n","        encoding = self.tokenizer(\n","            text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        item = {\n","            'input_ids': encoding['input_ids'].squeeze(0),\n","            'attention_mask': encoding['attention_mask'].squeeze(0)\n","        }\n","\n","        if self.labels is not None:\n","            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return item\n","\n","print(\"âœ“ Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 06. í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì™„ë²½í•œ ì „ì²˜ë¦¬\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"06. í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì™„ë²½í•œ ì „ì²˜ë¦¬\")\n","print(\"=\"*70)\n","\n","df_train = pd.read_csv(TRAIN_PATH)\n","print(f\"âœ“ ì›ë³¸ ë°ì´í„°: {len(df_train):,}ê°œ\")\n","\n","# ì™„ë²½í•œ ì „ì²˜ë¦¬ ì ìš©\n","df_train, preprocess_stats = preprocess_dataframe(\n","    df_train,\n","    min_length=MIN_TEXT_LENGTH,\n","    max_length=MAX_TEXT_LENGTH,\n","    verbose=True\n",")\n","\n","print(f\"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\")\n","class_distribution = df_train['class'].value_counts()\n","print(class_distribution)\n","print()\n","\n","# í´ë˜ìŠ¤ ê· í˜•ë„ ì²´í¬\n","max_count = class_distribution.max()\n","min_count = class_distribution.min()\n","imbalance_ratio = max_count / min_count\n","\n","print(f\"âš–ï¸  í´ë˜ìŠ¤ ê· í˜•ë„ ë¶„ì„:\")\n","print(f\"  ìµœëŒ€ í´ë˜ìŠ¤: {max_count:,}ê°œ\")\n","print(f\"  ìµœì†Œ í´ë˜ìŠ¤: {min_count:,}ê°œ\")\n","print(f\"  ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1\")\n","\n","if imbalance_ratio > 3:\n","    print(f\"  âš ï¸ ì‹¬ê°í•œ ë¶ˆê· í˜•! í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© í•„ìˆ˜\")\n","elif imbalance_ratio > 2:\n","    print(f\"  âš ï¸ ë¶ˆê· í˜• ìˆìŒ. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© ê¶Œì¥\")\n","else:\n","    print(f\"  âœ… ê· í˜• ì¡íŒ ë°ì´í„°ì…‹\")\n","print()\n","\n","# ============================================================================\n","# 07. ë¼ë²¨ ì¸ì½”ë”©\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"07. ë¼ë²¨ ì¸ì½”ë”©\")\n","print(\"=\"*70)\n","\n","label_encoder = LabelEncoder()\n","df_train['label'] = label_encoder.fit_transform(df_train['class'])\n","\n","print(f\"âœ“ í´ë˜ìŠ¤ ê°œìˆ˜: {len(label_encoder.classes_)}\")\n","print(f\"\\ní´ë˜ìŠ¤ ë§¤í•‘:\")\n","for i, class_name in enumerate(label_encoder.classes_):\n","    count = (df_train['class'] == class_name).sum()\n","    percentage = count / len(df_train) * 100\n","    print(f\"  {i}: {class_name:20s} - {count:5,}ê°œ ({percentage:5.1f}%)\")\n","print()\n","\n","# ============================================================================\n","# 08. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì¤‘ìš”!)\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"08. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)\")\n","print(\"=\"*70)\n","\n","# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n","class_weights_array = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(df_train['label']),\n","    y=df_train['label']\n",")\n","\n","# ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n","class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n","\n","# PyTorch í…ì„œë¡œ ë³€í™˜\n","class_weights_tensor = torch.FloatTensor(class_weights_array).to(DEVICE)\n","\n","print(f\"âœ“ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ\\n\")\n","print(f\"ğŸ“Š í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:\")\n","for i, (class_name, weight) in enumerate(zip(label_encoder.classes_, class_weights_array)):\n","    count = (df_train['label'] == i).sum()\n","    print(f\"  {class_name:20s}: ê°€ì¤‘ì¹˜ {weight:.4f} ({count:,}ê°œ)\")\n","\n","print(f\"\\nğŸ’¡ ê°€ì¤‘ì¹˜ íš¨ê³¼:\")\n","print(f\"  â€¢ ë°ì´í„°ê°€ ì ì€ í´ë˜ìŠ¤ â†’ ë†’ì€ ê°€ì¤‘ì¹˜ (ë” ì¤‘ìš”í•˜ê²Œ í•™ìŠµ)\")\n","print(f\"  â€¢ ë°ì´í„°ê°€ ë§ì€ í´ë˜ìŠ¤ â†’ ë‚®ì€ ê°€ì¤‘ì¹˜\")\n","print(f\"  â€¢ ë¶ˆê· í˜• ë¬¸ì œ ìë™ ë³´ì •!\")\n","print()\n","\n","# ============================================================================\n","# 09. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n","# ============================================================================\n","print(\"=\"*70)\n","print(\"09. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \")\n","print(\"=\"*70)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    df_train['conversation'].values,\n","    df_train['label'].values,\n","    test_size=VALIDATION_RATIO,\n","    random_state=42,\n","    stratify=df_train['label'].values\n",")\n","\n","print(f\"âœ“ í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê°œ ({(1-VALIDATION_RATIO)*100:.0f}%)\")\n","print(f\"âœ“ ê²€ì¦ ë°ì´í„°: {len(X_val):,}ê°œ ({VALIDATION_RATIO*100:.0f}%)\\n\")\n","\n","# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬\n","print(f\"ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬:\")\n","print(f\"  â€¢ í•™ìŠµ ë¼ë²¨ ë²”ìœ„: {y_train.min()} ~ {y_train.max()}\")\n","print(f\"  â€¢ ê²€ì¦ ë¼ë²¨ ë²”ìœ„: {y_val.min()} ~ {y_val.max()}\")\n","print(f\"  â€¢ í•™ìŠµ ë¼ë²¨ ìœ ë‹ˆí¬: {len(np.unique(y_train))}ê°œ\")\n","print(f\"  â€¢ ê²€ì¦ ë¼ë²¨ ìœ ë‹ˆí¬: {len(np.unique(y_val))}ê°œ\")\n","\n","# ìƒ˜í”Œ í™•ì¸\n","print(f\"\\nğŸ“ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):\")\n","for i in range(min(3, len(X_train))):\n","    print(f\"  {i+1}. Text: {X_train[i][:60]}...\")\n","    print(f\"     Label: {y_train[i]} ({label_encoder.classes_[y_train[i]]})\")\n","\n","print()\n","\n","# ============================================================================\n","# 10. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í˜¸í™˜ì„± ì²´í¬\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"10. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í˜¸í™˜ì„± ì²´í¬\")\n","print(\"=\"*70)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","config = AutoConfig.from_pretrained(MODEL_NAME)\n","model_max_length = config.max_position_embeddings\n","\n","print(f\"âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n","print(f\"âœ“ ëª¨ë¸ ìµœëŒ€ ìœ„ì¹˜ ì„ë² ë”©: {model_max_length}\")\n","print(f\"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ê¸¸ì´: {min(model_max_length, 512)}\\n\")\n","\n","SAFE_MAX_LENGTH = min(model_max_length, 512)\n","\n","# í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ì²´í¬ (ìƒ˜í”Œë§)\n","print(f\"ğŸ”§ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ì²´í¬ (ìƒ˜í”Œ 1000ê°œ)...\")\n","sample_size = min(1000, len(X_train))\n","sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n","sample_texts = X_train[sample_indices]\n","\n","over_length_count = 0\n","token_lengths = []\n","\n","for text in sample_texts:\n","    tokens = tokenizer.encode(text, add_special_tokens=True)\n","    token_lengths.append(len(tokens))\n","    if len(tokens) > SAFE_MAX_LENGTH:\n","        over_length_count += 1\n","\n","avg_token_length = np.mean(token_lengths)\n","max_token_length = np.max(token_lengths)\n","\n","print(f\"  â€¢ í‰ê·  í† í° ê¸¸ì´: {avg_token_length:.1f}\")\n","print(f\"  â€¢ ìµœëŒ€ í† í° ê¸¸ì´: {max_token_length}\")\n","print(f\"  â€¢ MAX_LENGTH({SAFE_MAX_LENGTH}) ì´ˆê³¼: {over_length_count}ê°œ ({over_length_count/sample_size*100:.1f}%)\")\n","\n","if over_length_count > sample_size * 0.1:\n","    print(f\"  âš ï¸ 10% ì´ìƒì´ MAX_LENGTH ì´ˆê³¼! 512 ì‚¬ìš© ê¶Œì¥\")\n","elif over_length_count > 0:\n","    print(f\"  âš ï¸ ì¼ë¶€ ë°ì´í„°ê°€ MAX_LENGTH ì´ˆê³¼. ì˜ë¦¼ í˜„ìƒ ë°œìƒ ê°€ëŠ¥\")\n","else:\n","    print(f\"  âœ… ëª¨ë“  ë°ì´í„°ê°€ MAX_LENGTH ì´ë‚´\")\n","print()\n","\n","# ============================================================================\n","# 11. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ í¬í•¨)\n","# ============================================================================\n","def create_model(num_labels):\n","    \"\"\"ëª¨ë¸ ìƒì„±\"\"\"\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_NAME,\n","        num_labels=num_labels\n","    ).to(DEVICE)\n","    return model\n","\n","print(\"=\"*70)\n","print(\"11. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# ============================================================================\n","# 12. ê²€ì¦ í•¨ìˆ˜ (Optuna ì‹œë„ìš©)\n","# ============================================================================\n","def evaluate_model(model, val_loader):\n","    \"\"\"ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ (Accuracy & F1 Score)\"\"\"\n","    model.eval()\n","    val_preds = []\n","    val_labels = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(DEVICE)\n","            attention_mask = batch['attention_mask'].to(DEVICE)\n","            labels = batch['labels'].to(DEVICE)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            predictions = torch.argmax(outputs.logits, dim=1)\n","\n","            val_preds.extend(predictions.cpu().numpy())\n","            val_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(val_labels, val_preds)\n","    f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n","\n","    return accuracy, f1\n","\n","# ============================================================================\n","# 13. ëª©ì  í•¨ìˆ˜ (Optunaìš©) - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\n","# ============================================================================\n","def objective(trial):\n","    \"\"\"Optuna ëª©ì  í•¨ìˆ˜ - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\"\"\"\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Trial {trial.number + 1}\")\n","    print(f\"{'='*70}\")\n","\n","    torch.cuda.empty_cache()\n","    import gc\n","    gc.collect()\n","\n","    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ\n","    batch_size = trial.suggest_categorical('batch_size', [8, 16])\n","    epochs = trial.suggest_int('epochs', 3, 5)\n","    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n","    max_length = trial.suggest_categorical('max_length', [256, 512])\n","\n","    print(f\"\\nğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n","    print(f\"  â€¢ Batch Size: {batch_size}\")\n","    print(f\"  â€¢ Epochs: {epochs}\")\n","    print(f\"  â€¢ Learning Rate: {learning_rate:.2e}\")\n","    print(f\"  â€¢ Max Length: {max_length}\")\n","\n","    if torch.cuda.is_available():\n","        print(f\"\\nğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\")\n","        print(f\"  â€¢ í• ë‹¹ë¨: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n","        print(f\"  â€¢ ì˜ˆì•½ë¨: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n","\n","    try:\n","        # Dataset ìƒì„±\n","        train_dataset = ConversationDataset(\n","            X_train, tokenizer, labels=y_train, max_length=max_length\n","        )\n","        val_dataset = ConversationDataset(\n","            X_val, tokenizer, labels=y_val, max_length=max_length\n","        )\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","        # ëª¨ë¸ ìƒì„±\n","        model = create_model(len(label_encoder.classes_))\n","        optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","        # ì†ì‹¤ í•¨ìˆ˜ì— í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\n","        criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n","\n","        # í•™ìŠµ\n","        print(f\"\\nğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\")\n","        best_val_accuracy = 0\n","        best_val_f1 = 0\n","\n","        for epoch in range(epochs):\n","            model.train()\n","            total_loss = 0\n","            train_correct = 0\n","            train_total = 0\n","\n","            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n","            for batch_idx, batch in enumerate(pbar):\n","                input_ids = batch['input_ids'].to(DEVICE)\n","                attention_mask = batch['attention_mask'].to(DEVICE)\n","                labels = batch['labels'].to(DEVICE)\n","\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                )\n","\n","                # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ ê³„ì‚°\n","                loss = criterion(outputs.logits, labels)\n","\n","                if torch.isnan(loss):\n","                    print(f\"\\nâš ï¸ NaN loss ë°œê²¬! Epoch {epoch+1}, Batch {batch_idx}\")\n","                    raise ValueError(\"Loss became NaN\")\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                predictions = torch.argmax(outputs.logits, dim=1)\n","                train_correct += (predictions == labels).sum().item()\n","                train_total += labels.size(0)\n","\n","                pbar.set_postfix({\n","                    'loss': f'{total_loss/(batch_idx+1):.4f}',\n","                    'acc': f'{train_correct/train_total:.4f}'\n","                })\n","\n","            train_accuracy = train_correct / train_total\n","            avg_train_loss = total_loss / len(train_loader)\n","\n","            # ê²€ì¦\n","            val_accuracy, val_f1 = evaluate_model(model, val_loader)\n","\n","            print(f\"  Epoch {epoch+1}/{epochs}:\")\n","            print(f\"    Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n","            print(f\"    Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n","\n","            if val_accuracy > best_val_accuracy:\n","                best_val_accuracy = val_accuracy\n","                best_val_f1 = val_f1\n","\n","            # ì¡°ê¸° ì¢…ë£Œ\n","            trial.report(best_val_accuracy, epoch)\n","            if trial.should_prune():\n","                print(f\"  âš ï¸ Trial pruned at epoch {epoch+1}\")\n","                raise optuna.TrialPruned()\n","\n","        print(f\"\\nâœ… Trial {trial.number + 1} ì™„ë£Œ\")\n","        print(f\"   Best Val Accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\n","        print(f\"   Best Val F1: {best_val_f1:.4f}\")\n","\n","        return best_val_accuracy\n","\n","    except optuna.TrialPruned:\n","        raise\n","    except Exception as e:\n","        print(f\"\\nâŒ Trial {trial.number + 1} ì‹¤íŒ¨: {str(e)}\")\n","        import traceback\n","        print(f\"ìƒì„¸ ì—ëŸ¬:\\n{traceback.format_exc()}\")\n","        return 0.0\n","\n","    finally:\n","        print(f\"\\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\")\n","        try:\n","            del model\n","            del optimizer\n","            del criterion\n","            del train_loader\n","            del val_loader\n","            del train_dataset\n","            del val_dataset\n","        except:\n","            pass\n","\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        if torch.cuda.is_available():\n","            print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\")\n","            print(f\"  â€¢ í• ë‹¹ë¨: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n","            print(f\"  â€¢ ì˜ˆì•½ë¨: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n","\n","print(\"=\"*70)\n","print(\"12. ëª©ì  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\")\n","print(\"=\"*70 + \"\\n\")\n","\n","# ============================================================================\n","# 14. Optuna ìµœì í™” ì‹¤í–‰\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"13. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘\")\n","print(\"=\"*70)\n","\n","sampler = optuna.samplers.TPESampler(seed=42)\n","study = optuna.create_study(\n","    direction='maximize',\n","    sampler=sampler,\n","    pruner=optuna.pruners.MedianPruner()\n",")\n","\n","start_tuning_time = time.time()\n","study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_PER_TRIAL)\n","tuning_time = time.time() - start_tuning_time\n","\n","print(f\"\\n{'='*70}\")\n","print(f\"âœ… Optuna íŠœë‹ ì™„ë£Œ!\")\n","print(f\"{'='*70}\")\n","\n","# ============================================================================\n","# 15. ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸\n","# ============================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"14. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°\")\n","print(\"=\"*70)\n","\n","best_trial = study.best_trial\n","\n","print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ Trial: #{best_trial.number}\")\n","print(f\"   ê²€ì¦ ì •í™•ë„: {best_trial.value:.4f} ({best_trial.value*100:.2f}%)\")\n","print(f\"\\nğŸ“Š ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n","for param_name, param_value in best_trial.params.items():\n","    print(f\"  â€¢ {param_name}: {param_value}\")\n","\n","best_batch_size = best_trial.params['batch_size']\n","best_epochs = best_trial.params['epochs']\n","best_learning_rate = best_trial.params['learning_rate']\n","best_max_length = best_trial.params['max_length']\n","\n","# ============================================================================\n","# 16. ëª¨ë“  Trial ê²°ê³¼ ìš”ì•½\n","# ============================================================================\n","print(f\"\\n{'='*70}\")\n","print(\"15. ëª¨ë“  Trial ê²°ê³¼\")\n","print(f\"{'='*70}\\n\")\n","\n","trials_df = study.trials_dataframe()\n","print(f\"ì´ ì‹œë„ íšŸìˆ˜: {len(study.trials)}\")\n","print(f\"ì„±ê³µí•œ Trial: {len([t for t in study.trials if t.state == TrialState.COMPLETE])}\")\n","print(f\"Pruned Trial: {len([t for t in study.trials if t.state == TrialState.PRUNED])}\")\n","\n","print(f\"\\nìµœìƒìœ„ 5ê°œ Trial:\")\n","print(trials_df[['number', 'value', 'params_batch_size', 'params_epochs',\n","                 'params_learning_rate', 'params_max_length']].sort_values('value', ascending=False).head(5).to_string())\n","\n","# ============================================================================\n","# 17. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\n","# ============================================================================\n","print(f\"\\n{'='*70}\")\n","print(\"16. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\")\n","print(f\"{'='*70}\\n\")\n","\n","print(f\"ğŸ“ í•™ìŠµ ì„¤ì •:\")\n","print(f\"  â€¢ Batch Size: {best_batch_size}\")\n","print(f\"  â€¢ Epochs: {best_epochs}\")\n","print(f\"  â€¢ Learning Rate: {best_learning_rate:.2e}\")\n","print(f\"  â€¢ Max Length: {best_max_length}\")\n","print(f\"  â€¢ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì ìš© âœ…\\n\")\n","\n","# Dataset ìƒì„±\n","final_train_dataset = ConversationDataset(\n","    X_train, tokenizer, labels=y_train, max_length=best_max_length\n",")\n","final_val_dataset = ConversationDataset(\n","    X_val, tokenizer, labels=y_val, max_length=best_max_length\n",")\n","\n","final_train_loader = DataLoader(final_train_dataset, batch_size=best_batch_size, shuffle=True)\n","final_val_loader = DataLoader(final_val_dataset, batch_size=best_batch_size)\n","\n","# ìµœì¢… ëª¨ë¸ ìƒì„±\n","final_model = create_model(len(label_encoder.classes_))\n","final_optimizer = AdamW(final_model.parameters(), lr=best_learning_rate)\n","\n","# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜\n","final_criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n","\n","# ìµœì¢… í•™ìŠµ\n","print(\"ğŸ”„ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...\\n\")\n","\n","best_final_accuracy = 0\n","training_history_final = []\n","\n","for epoch in range(best_epochs):\n","    final_model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    pbar = tqdm(final_train_loader, desc=f\"Epoch {epoch+1}/{best_epochs}\")\n","\n","    for batch in pbar:\n","        input_ids = batch['input_ids'].to(DEVICE)\n","        attention_mask = batch['attention_mask'].to(DEVICE)\n","        labels = batch['labels'].to(DEVICE)\n","\n","        outputs = final_model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","        )\n","\n","        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©ëœ ì†ì‹¤\n","        loss = final_criterion(outputs.logits, labels)\n","\n","        final_optimizer.zero_grad()\n","        loss.backward()\n","        final_optimizer.step()\n","\n","        total_loss += loss.item()\n","        predictions = torch.argmax(outputs.logits, dim=1)\n","        correct += (predictions == labels).sum().item()\n","        total += labels.size(0)\n","\n","        pbar.set_postfix({\n","            'loss': f'{total_loss/len(pbar):.4f}',\n","            'acc': f'{correct/total:.4f}'\n","        })\n","\n","    train_avg_loss = total_loss / len(final_train_loader)\n","    train_accuracy = correct / total\n","\n","    # ê²€ì¦\n","    final_model.eval()\n","    val_preds = []\n","    val_labels = []\n","\n","    pbar_val = tqdm(final_val_loader, desc=f\"Epoch {epoch+1}/{best_epochs} - Val\", leave=False)\n","\n","    with torch.no_grad():\n","        for batch in pbar_val:\n","            input_ids = batch['input_ids'].to(DEVICE)\n","            attention_mask = batch['attention_mask'].to(DEVICE)\n","            labels = batch['labels'].to(DEVICE)\n","\n","            outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n","            predictions = torch.argmax(outputs.logits, dim=1)\n","\n","            val_preds.extend(predictions.cpu().numpy())\n","            val_labels.extend(labels.cpu().numpy())\n","\n","    val_accuracy = accuracy_score(val_labels, val_preds)\n","    val_f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n","\n","    training_history_final.append({\n","        'epoch': epoch + 1,\n","        'train_loss': train_avg_loss,\n","        'train_acc': train_accuracy,\n","        'val_acc': val_accuracy,\n","        'val_f1': val_f1\n","    })\n","\n","    print(f\"âœ“ Epoch {epoch+1}/{best_epochs} - Train Loss: {train_avg_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n","\n","    if val_accuracy > best_final_accuracy:\n","        best_final_accuracy = val_accuracy\n","\n","print(f\"\\nâœ“ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n","print(f\"  ìµœê³  ê²€ì¦ ì •í™•ë„: {best_final_accuracy:.4f} ({best_final_accuracy*100:.2f}%)\\n\")\n","\n","# ============================================================================\n","# 18. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"17. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\")\n","print(\"=\"*70)\n","\n","df_test = pd.read_csv(TEST_PATH)\n","print(f\"âœ“ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(df_test):,}ê°œ\")\n","\n","# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ì „ì²˜ë¦¬ (ë¼ë²¨ ì—†ìŒ)\n","df_test['conversation'] = df_test['conversation'].apply(clean_text)\n","print(f\"âœ“ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 19. í…ŒìŠ¤íŠ¸ Dataset ë° DataLoader ìƒì„±\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"18. í…ŒìŠ¤íŠ¸ Dataset ìƒì„±\")\n","print(\"=\"*70)\n","\n","test_dataset = ConversationDataset(\n","    df_test['conversation'].values,\n","    tokenizer,\n","    labels=None,\n","    max_length=best_max_length\n",")\n","test_loader = DataLoader(test_dataset, batch_size=best_batch_size)\n","\n","print(f\"âœ“ í…ŒìŠ¤íŠ¸ DataLoader ìƒì„± ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 20. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜ (ì‹ ë¢°ë„ 3ë‹¨ê³„ + Lowâ†’ì¼ë°˜ëŒ€í™”)\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"19. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜ (ìµœì í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’)\")\n","print(\"=\"*70 + \"\\n\")\n","\n","print(\"ğŸ“Š ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ ì„¤ì • (ìµœì í™”ë¨):\")\n","print(f\"  â€¢ High: â‰¥ {CONFIDENCE_THRESHOLD_HIGH} - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\")\n","print(f\"  â€¢ Mid: {CONFIDENCE_THRESHOLD_MID} ~ {CONFIDENCE_THRESHOLD_HIGH} - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\")\n","print(f\"  â€¢ Low: < {CONFIDENCE_THRESHOLD_MID} - 'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜\")\n","print(f\"  â€¢ Top-K ì˜ˆì¸¡ ê°œìˆ˜: {TOP_K_PREDICTIONS}\\n\")\n","\n","# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜\n","final_model.eval()\n","\n","predictions = []\n","original_predictions = []\n","confidences = []\n","confidence_levels = []\n","reclassified_flags = []\n","top_k_predictions_list = []\n","top_k_confidences_list = []\n","\n","print(\"ğŸ”„ ë¶„ë¥˜ ì¤‘...\\n\")\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader):\n","        input_ids = batch['input_ids'].to(DEVICE)\n","        attention_mask = batch['attention_mask'].to(DEVICE)\n","\n","        outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        probs = torch.softmax(logits, dim=1)\n","\n","        # Top-1 ì˜ˆì¸¡\n","        pred_ids = torch.argmax(logits, dim=1)\n","        pred_classes = [label_encoder.classes_[pid] for pid in pred_ids.cpu().numpy()]\n","        conf_scores = torch.max(probs, dim=1)[0].cpu().numpy()\n","\n","        # Top-K ì˜ˆì¸¡\n","        if USE_TOP_K:\n","            top_k_probs, top_k_indices = torch.topk(probs, k=min(TOP_K_PREDICTIONS, probs.size(1)), dim=1)\n","\n","            for i in range(len(top_k_indices)):\n","                top_k_classes = [label_encoder.classes_[idx] for idx in top_k_indices[i].cpu().numpy()]\n","                top_k_confs = top_k_probs[i].cpu().numpy()\n","\n","                top_k_predictions_list.append(top_k_classes)\n","                top_k_confidences_list.append(top_k_confs)\n","        else:\n","            top_k_predictions_list.extend([None] * len(pred_classes))\n","            top_k_confidences_list.extend([None] * len(pred_classes))\n","\n","        # ì‹ ë¢°ë„ ê¸°ë°˜ ì²˜ë¦¬ (3ë‹¨ê³„)\n","        for pred_class, conf in zip(pred_classes, conf_scores):\n","            if conf >= CONFIDENCE_THRESHOLD_HIGH:\n","                conf_level = \"High\"\n","                final_pred = pred_class\n","                reclassified = False\n","            elif conf >= CONFIDENCE_THRESHOLD_MID:\n","                conf_level = \"Mid\"\n","                final_pred = pred_class\n","                reclassified = False\n","            else:\n","                conf_level = \"Low\"\n","                final_pred = \"ì¼ë°˜ ëŒ€í™”\"\n","                reclassified = True\n","\n","            predictions.append(final_pred)\n","            original_predictions.append(pred_class)\n","            confidences.append(conf)\n","            confidence_levels.append(conf_level)\n","            reclassified_flags.append(reclassified)\n","\n","print(\"\\nâœ“ ë¶„ë¥˜ ì™„ë£Œ!\\n\")\n","\n","# ============================================================================\n","# ë¶„ë¥˜ ê²°ê³¼ í†µê³„\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"ğŸ“Š ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ í†µê³„\")\n","print(\"=\"*70 + \"\\n\")\n","\n","conf_level_counts = Counter(confidence_levels)\n","print(\"ì‹ ë¢°ë„ ë“±ê¸‰ë³„ ë¶„í¬:\")\n","for level in [\"High\", \"Mid\", \"Low\"]:\n","    if level in conf_level_counts:\n","        count = conf_level_counts[level]\n","        percentage = (count / len(predictions)) * 100\n","        print(f\"  {level:4s}: {count:4d}ê°œ ({percentage:5.1f}%)\")\n","\n","reclassified_count = sum(reclassified_flags)\n","print(f\"\\n'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜:\")\n","print(f\"  {reclassified_count:4d}ê°œ ({reclassified_count/len(predictions)*100:5.1f}%)\")\n","\n","if reclassified_count > 0:\n","    reclassified_original = [original_predictions[i] for i, flag in enumerate(reclassified_flags) if flag]\n","    original_class_counts = Counter(reclassified_original)\n","    print(f\"\\nì¬ë¶„ë¥˜ëœ í•­ëª©ì˜ ì›ë³¸ ì˜ˆì¸¡ ë¶„í¬:\")\n","    for class_name, count in original_class_counts.most_common():\n","        print(f\"  {class_name:20s}: {count:3d}ê°œ\")\n","\n","print()\n","\n","# ============================================================================\n","# 21. ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"20. ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\")\n","print(\"=\"*70)\n","\n","df_test['prediction'] = predictions\n","df_test['original_prediction'] = original_predictions\n","df_test['confidence'] = confidences\n","df_test['confidence_level'] = confidence_levels\n","df_test['reclassified_to_normal'] = reclassified_flags\n","\n","if USE_TOP_K:\n","    df_test['top_k_predictions'] = [\n","        ' | '.join([f\"{cls}({conf:.3f})\" for cls, conf in zip(preds, confs)])\n","        if preds is not None else ''\n","        for preds, confs in zip(top_k_predictions_list, top_k_confidences_list)\n","    ]\n","\n","    for k in range(TOP_K_PREDICTIONS):\n","        df_test[f'pred_rank_{k+1}'] = [\n","            preds[k] if preds is not None and k < len(preds) else ''\n","            for preds in top_k_predictions_list\n","        ]\n","        df_test[f'conf_rank_{k+1}'] = [\n","            f\"{confs[k]:.4f}\" if confs is not None and k < len(confs) else ''\n","            for confs in top_k_confidences_list\n","        ]\n","\n","print(f\"âœ“ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ\\n\")\n","\n","# ============================================================================\n","# 22. ê²°ê³¼ ì €ì¥\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"21. ê²°ê³¼ ì €ì¥ (Google Drive)\")\n","print(\"=\"*70)\n","\n","OUTPUT_PATH_DETAILED = '/content/drive/My Drive/predictions_detailed.csv'\n","df_test.to_csv(OUTPUT_PATH_DETAILED, index=False, encoding='utf-8')\n","print(f\"âœ“ ìƒì„¸ ë²„ì „ ì €ì¥: predictions_detailed.csv\")\n","\n","OUTPUT_PATH_SIMPLE = '/content/drive/My Drive/predictions_simple.csv'\n","df_test[['idx', 'conversation', 'prediction', 'confidence']].to_csv(\n","    OUTPUT_PATH_SIMPLE, index=False, encoding='utf-8'\n",")\n","print(f\"âœ“ ê°„ë‹¨ ë²„ì „ ì €ì¥: predictions_simple.csv\")\n","\n","OUTPUT_PATH_RECLASSIFIED = '/content/drive/My Drive/predictions_reclassified.csv'\n","df_reclassified = df_test[df_test['reclassified_to_normal'] == True]\n","df_reclassified.to_csv(OUTPUT_PATH_RECLASSIFIED, index=False, encoding='utf-8')\n","print(f\"âœ“ ì¬ë¶„ë¥˜ ë²„ì „ ì €ì¥: predictions_reclassified.csv ({len(df_reclassified)}ê°œ)\")\n","\n","OUTPUT_PATH_TRUSTED = '/content/drive/My Drive/predictions_trusted.csv'\n","df_trusted = df_test[df_test['reclassified_to_normal'] == False]\n","df_trusted.to_csv(OUTPUT_PATH_TRUSTED, index=False, encoding='utf-8')\n","print(f\"âœ“ ì‹ ë¢° ë²„ì „ ì €ì¥: predictions_trusted.csv ({len(df_trusted)}ê°œ)\")\n","\n","print()\n","\n","# ============================================================================\n","# ì‹ ë¢°ë„ ì‹œê°í™”\n","# ============================================================================\n","def visualize_confidence_distribution():\n","    \"\"\"ì‹ ë¢°ë„ ë¶„í¬ ì‹œê°í™”\"\"\"\n","    plt.rcParams['font.family'] = 'DejaVu Sans'\n","    plt.rcParams['axes.unicode_minus'] = False\n","\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # 1. ì‹ ë¢°ë„ íˆìŠ¤í† ê·¸ë¨\n","    axes[0, 0].hist(df_test['confidence'], bins=50, color='skyblue', edgecolor='black')\n","    axes[0, 0].axvline(CONFIDENCE_THRESHOLD_MID, color='orange', linestyle='--',\n","                       label=f'Mid/Low ({CONFIDENCE_THRESHOLD_MID})')\n","    axes[0, 0].axvline(CONFIDENCE_THRESHOLD_HIGH, color='green', linestyle='--',\n","                       label=f'High/Mid ({CONFIDENCE_THRESHOLD_HIGH})')\n","    axes[0, 0].set_title('Confidence Score Distribution', fontsize=12, fontweight='bold')\n","    axes[0, 0].set_xlabel('Confidence Score')\n","    axes[0, 0].set_ylabel('Frequency')\n","    axes[0, 0].legend()\n","    axes[0, 0].grid(axis='y', alpha=0.3)\n","\n","    # 2. ì‹ ë¢°ë„ ë“±ê¸‰ë³„ ë¶„í¬\n","    level_data = df_test['confidence_level'].value_counts()\n","    colors = {'High': 'green', 'Mid': 'yellow', 'Low': 'red'}\n","    colors_list = [colors.get(level, 'gray') for level in level_data.index]\n","\n","    axes[0, 1].bar(range(len(level_data)), level_data.values, color=colors_list, edgecolor='black')\n","    axes[0, 1].set_xticks(range(len(level_data)))\n","    axes[0, 1].set_xticklabels(level_data.index, rotation=0)\n","    axes[0, 1].set_title('Confidence Level Distribution', fontsize=12, fontweight='bold')\n","    axes[0, 1].set_ylabel('Count')\n","    axes[0, 1].grid(axis='y', alpha=0.3)\n","\n","    # 3. ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤ë³„ ë¶„í¬\n","    pred_counts = df_test['prediction'].value_counts()\n","    axes[1, 0].barh(range(len(pred_counts)), pred_counts.values, color='lightcoral')\n","    axes[1, 0].set_yticks(range(len(pred_counts)))\n","    axes[1, 0].set_yticklabels([str(c)[:20] for c in pred_counts.index], fontsize=8)\n","    axes[1, 0].set_title('Final Prediction Distribution', fontsize=12, fontweight='bold')\n","    axes[1, 0].set_xlabel('Count')\n","    axes[1, 0].grid(axis='x', alpha=0.3)\n","\n","    # 4. ì¬ë¶„ë¥˜ ì—¬ë¶€\n","    reclass_counts = df_test['reclassified_to_normal'].value_counts()\n","    colors_reclass = ['green', 'orange']\n","    labels_reclass = ['Original Prediction', 'Reclassified to Normal']\n","    axes[1, 1].pie(reclass_counts.values, labels=labels_reclass, autopct='%1.1f%%',\n","                   colors=colors_reclass, startangle=90)\n","    axes[1, 1].set_title('Reclassification Status', fontsize=12, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig('/content/drive/My Drive/confidence_analysis.png', dpi=300, bbox_inches='tight')\n","    print(\"âœ“ ì‹ ë¢°ë„ ë¶„ì„ ì‹œê°í™” ì €ì¥: confidence_analysis.png\\n\")\n","\n","try:\n","    visualize_confidence_distribution()\n","except Exception as e:\n","    print(f\"âš ï¸ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\\n\")\n","\n","# ============================================================================\n","# 23. ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€\n","# ============================================================================\n","print(\"\\n\\n\" + \"â–ˆ\"*70)\n","print(\"â–ˆ\" + \" \"*68 + \"â–ˆ\")\n","print(\"â–ˆ\" + \"  âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\".center(68) + \"â–ˆ\")\n","print(\"â–ˆ\" + \" \"*68 + \"â–ˆ\")\n","print(\"â–ˆ\"*70)\n","\n","print(\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼ ì •ë³´:\")\n","print(f\"\\n  ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ (4ê°€ì§€ ë²„ì „):\")\n","print(f\"     1. predictions_detailed.csv - ì „ì²´ ìƒì„¸ ({len(df_test)}ê°œ)\")\n","print(f\"     2. predictions_simple.csv - ìµœì¢… ì˜ˆì¸¡ë§Œ ({len(df_test)}ê°œ)\")\n","print(f\"     3. predictions_reclassified.csv - 'ì¼ë°˜ ëŒ€í™”' ì¬ë¶„ë¥˜ ({len(df_reclassified)}ê°œ)\")\n","print(f\"     4. predictions_trusted.csv - High+Mid ì‹ ë¢°ë„ ({len(df_trusted)}ê°œ)\")\n","\n","print(f\"\\n  ğŸ“ˆ ì‹œê°í™”:\")\n","print(f\"     5. confidence_analysis.png - ì‹ ë¢°ë„ ë¶„ì„\")\n","\n","print(f\"\\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n","print(f\"  â€¢ Batch Size: {best_batch_size}\")\n","print(f\"  â€¢ Epochs: {best_epochs}\")\n","print(f\"  â€¢ Learning Rate: {best_learning_rate:.2e}\")\n","print(f\"  â€¢ Max Length: {best_max_length}\")\n","\n","print(f\"\\nğŸ“ˆ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:\")\n","print(f\"  â€¢ ê²€ì¦ ì •í™•ë„: {best_final_accuracy:.4f} ({best_final_accuracy*100:.2f}%)\")\n","\n","print(f\"\\nğŸ¯ ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ í†µê³„:\")\n","print(f\"  â€¢ High (â‰¥{CONFIDENCE_THRESHOLD_HIGH}): {conf_level_counts.get('High', 0)}ê°œ ({conf_level_counts.get('High', 0)/len(predictions)*100:.1f}%)\")\n","print(f\"  â€¢ Mid ({CONFIDENCE_THRESHOLD_MID}~{CONFIDENCE_THRESHOLD_HIGH}): {conf_level_counts.get('Mid', 0)}ê°œ ({conf_level_counts.get('Mid', 0)/len(predictions)*100:.1f}%)\")\n","print(f\"  â€¢ Low (<{CONFIDENCE_THRESHOLD_MID}): {conf_level_counts.get('Low', 0)}ê°œ ({conf_level_counts.get('Low', 0)/len(predictions)*100:.1f}%) â†’ 'ì¼ë°˜ ëŒ€í™”' ì¬ë¶„ë¥˜\")\n","\n","print(f\"\\nğŸ”¥ ì ìš©ëœ ê°œì„ ì‚¬í•­:\")\n","print(f\"  âœ… ì™„ë²½í•œ ì „ì²˜ë¦¬ (BOM, ê³µë°±, ë”°ì˜´í‘œ, ê¸¸ì´)\")\n","print(f\"  âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© (ë¶ˆê· í˜• í•´ê²°)\")\n","print(f\"  âœ… ì‹ ë¢°ë„ ì„ê³„ê°’ ìµœì í™” (0.8â†’0.6, 0.9â†’0.85)\")\n","print(f\"  âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦\")\n","print(f\"  âœ… í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ì²´í¬\")\n","\n","print(f\"\\nâ±ï¸  ìˆ˜í–‰ ì‹œê°„:\")\n","print(f\"  â€¢ Optuna íŠœë‹: {tuning_time:.1f}ì´ˆ ({tuning_time/60:.1f}ë¶„)\")\n","\n","print(f\"\\nğŸ’¾ ëª¨ë“  íŒŒì¼ì´ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n","print(\"\\n\" + \"â–ˆ\"*70 + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mz_y2U8O2JfY","executionInfo":{"status":"ok","timestamp":1762799020075,"user_tz":-540,"elapsed":2405293,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"}},"outputId":"cfe785f6-7360-4178-a39a-c6ae69623e62"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","01. GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ\n","======================================================================\n","\n","======================================================================\n","02. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","======================================================================\n","âœ“ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\n","\n","======================================================================\n","03. ê¸°ë³¸ ì„¤ì •\n","======================================================================\n","âœ“ ëª¨ë¸: beomi/kcbert-base\n","âœ“ ë””ë°”ì´ìŠ¤: cuda\n","âœ“ Optuna ì‹œë„ íšŸìˆ˜: 5\n","âœ“ Validation ë¹„ìœ¨: 20.0%\n","\n","âœ“ ì‹ ë¢°ë„ ì„ê³„ê°’ (3ë‹¨ê³„) - ìµœì í™”:\n","  â€¢ High: â‰¥ 0.85\n","  â€¢ Mid: 0.6 ~ 0.85\n","  â€¢ Low: < 0.6 â†’ 'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜\n","\n","âœ“ ì „ì²˜ë¦¬ ì„¤ì •:\n","  â€¢ ìµœì†Œ ê¸¸ì´: 10ì\n","  â€¢ ìµœëŒ€ ê¸¸ì´: 1500ì\n","\n","ğŸ’¾ GPU ì •ë³´:\n","  â€¢ GPU: Tesla T4\n","  â€¢ ì´ ë©”ëª¨ë¦¬: 14.74 GB\n","\n","======================================================================\n","04. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n","======================================================================\n","âœ“ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n","\n","======================================================================\n","05. Dataset í´ë˜ìŠ¤ ì •ì˜\n","======================================================================\n","âœ“ Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n","\n","======================================================================\n","06. í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì™„ë²½í•œ ì „ì²˜ë¦¬\n","======================================================================\n","âœ“ ì›ë³¸ ë°ì´í„°: 4,950ê°œ\n","\n","ğŸ§¹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n","  ì›ë³¸ ë°ì´í„°: 4,950ê°œ\n","  âœ“ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘ (BOM, ê³µë°±, ë”°ì˜´í‘œ, íŠ¹ìˆ˜ë¬¸ì)...\n","  âœ“ ì¤‘ë³µ ì œê±°: 105ê°œ\n","\n","  âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\n","  ìµœì¢… ë°ì´í„°: 4,845ê°œ (ìœ ì§€ìœ¨: 97.9%)\n","  ì´ ì œê±°: 105ê°œ\n","\n","ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\n","class\n","ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”      1011\n","ì¼ë°˜ ëŒ€í™”           999\n","ê°ˆì·¨ ëŒ€í™”           973\n","ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”     970\n","í˜‘ë°• ëŒ€í™”           892\n","Name: count, dtype: int64\n","\n","âš–ï¸  í´ë˜ìŠ¤ ê· í˜•ë„ ë¶„ì„:\n","  ìµœëŒ€ í´ë˜ìŠ¤: 1,011ê°œ\n","  ìµœì†Œ í´ë˜ìŠ¤: 892ê°œ\n","  ë¶ˆê· í˜• ë¹„ìœ¨: 1.13:1\n","  âœ… ê· í˜• ì¡íŒ ë°ì´í„°ì…‹\n","\n","======================================================================\n","07. ë¼ë²¨ ì¸ì½”ë”©\n","======================================================================\n","âœ“ í´ë˜ìŠ¤ ê°œìˆ˜: 5\n","\n","í´ë˜ìŠ¤ ë§¤í•‘:\n","  0: ê°ˆì·¨ ëŒ€í™”                -   973ê°œ ( 20.1%)\n","  1: ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”            - 1,011ê°œ ( 20.9%)\n","  2: ì¼ë°˜ ëŒ€í™”                -   999ê°œ ( 20.6%)\n","  3: ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”          -   970ê°œ ( 20.0%)\n","  4: í˜‘ë°• ëŒ€í™”                -   892ê°œ ( 18.4%)\n","\n","======================================================================\n","08. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)\n","======================================================================\n","âœ“ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ\n","\n","ğŸ“Š í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:\n","  ê°ˆì·¨ ëŒ€í™”               : ê°€ì¤‘ì¹˜ 0.9959 (973ê°œ)\n","  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”           : ê°€ì¤‘ì¹˜ 0.9585 (1,011ê°œ)\n","  ì¼ë°˜ ëŒ€í™”               : ê°€ì¤‘ì¹˜ 0.9700 (999ê°œ)\n","  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”         : ê°€ì¤‘ì¹˜ 0.9990 (970ê°œ)\n","  í˜‘ë°• ëŒ€í™”               : ê°€ì¤‘ì¹˜ 1.0863 (892ê°œ)\n","\n","ğŸ’¡ ê°€ì¤‘ì¹˜ íš¨ê³¼:\n","  â€¢ ë°ì´í„°ê°€ ì ì€ í´ë˜ìŠ¤ â†’ ë†’ì€ ê°€ì¤‘ì¹˜ (ë” ì¤‘ìš”í•˜ê²Œ í•™ìŠµ)\n","  â€¢ ë°ì´í„°ê°€ ë§ì€ í´ë˜ìŠ¤ â†’ ë‚®ì€ ê°€ì¤‘ì¹˜\n","  â€¢ ë¶ˆê· í˜• ë¬¸ì œ ìë™ ë³´ì •!\n","\n","======================================================================\n","09. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n","======================================================================\n","âœ“ í•™ìŠµ ë°ì´í„°: 3,876ê°œ (80%)\n","âœ“ ê²€ì¦ ë°ì´í„°: 969ê°œ (20%)\n","\n","ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬:\n","  â€¢ í•™ìŠµ ë¼ë²¨ ë²”ìœ„: 0 ~ 4\n","  â€¢ ê²€ì¦ ë¼ë²¨ ë²”ìœ„: 0 ~ 4\n","  â€¢ í•™ìŠµ ë¼ë²¨ ìœ ë‹ˆí¬: 5ê°œ\n","  â€¢ ê²€ì¦ ë¼ë²¨ ìœ ë‹ˆí¬: 5ê°œ\n","\n","ğŸ“ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):\n","  1. Text: ë”¸ ì•„ë¹  50ë§Œì›ë§Œ ë¹Œë ¤ì¤˜\n"," ì•ˆë¼ ë‚˜ë„ ëˆ ì—†ì–´\n"," ì–´ë””ì„œ ë§ëŒ€ê¾¸ì•¼ ì•„ë¹ ê°€ ì£¼ë¼ë©´ ì¤˜ì•¼ì§€\n"," ë‚˜ë„ ì§„ì§œ ì–¼ë§ˆ ì—†...\n","     Label: 0 (ê°ˆì·¨ ëŒ€í™”)\n","  2. Text: ì–´ì´ ë‹¤ë“¤ ë˜‘ë°”ë¡œ ì•ˆí•´?\n","ì‚¬ì¥ë‹˜.ì œë°œ ë¬¼ ì¢€ ì£¼ì„¸ìš”.\n","ë­? ë¬¼? ì´ê²Œ ë¯¸ì³¤ë‚˜. ì•¼ ì§€ê¸ˆ ê³ ì‘ ì´ê±° ì¡°ê¸ˆ ë§Œë“¤...\n","     Label: 3 (ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”)\n","  3. Text: ì´ê±° ë„ì™€ì£¼ì„¸ìš”.\n"," ë‚œ ì§€ê¸ˆ ë°”ë¹ ì„œ ë„ì™€ì¤„ ì‹œê°„ì´ ì—†ì–´.\n"," ë„Œ ë°”ì  ë•Œ í•­ìƒ ë°”ìœì²™ í•˜ë”ë¼\n"," ë°”ìœì²™ í•˜ëŠ” ê²Œ...\n","     Label: 4 (í˜‘ë°• ëŒ€í™”)\n","\n","======================================================================\n","10. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í˜¸í™˜ì„± ì²´í¬\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (321 > 300). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n","âœ“ ëª¨ë¸ ìµœëŒ€ ìœ„ì¹˜ ì„ë² ë”©: 300\n","âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ ê¸¸ì´: 300\n","\n","ğŸ”§ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ì²´í¬ (ìƒ˜í”Œ 1000ê°œ)...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-11-10 17:43:49,016] A new study created in memory with name: no-name-af7d57df-d90f-4f27-b080-909740ada74d\n"]},{"output_type":"stream","name":"stdout","text":["  â€¢ í‰ê·  í† í° ê¸¸ì´: 110.3\n","  â€¢ ìµœëŒ€ í† í° ê¸¸ì´: 328\n","  â€¢ MAX_LENGTH(300) ì´ˆê³¼: 3ê°œ (0.3%)\n","  âš ï¸ ì¼ë¶€ ë°ì´í„°ê°€ MAX_LENGTH ì´ˆê³¼. ì˜ë¦¼ í˜„ìƒ ë°œìƒ ê°€ëŠ¥\n","\n","======================================================================\n","11. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n","======================================================================\n","\n","======================================================================\n","12. ëª©ì  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\n","======================================================================\n","\n","======================================================================\n","13. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘\n","======================================================================\n","\n","======================================================================\n","Trial 1\n","======================================================================\n","\n","ğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 16\n","  â€¢ Epochs: 5\n","  â€¢ Learning Rate: 2.62e-05\n","  â€¢ Max Length: 256\n","\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 1/5:\n","    Train Loss: 0.4710 | Train Acc: 0.8295\n","    Val Acc: 0.9020 | Val F1: 0.9004\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 2/5:\n","    Train Loss: 0.1685 | Train Acc: 0.9543\n","    Val Acc: 0.9071 | Val F1: 0.9076\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 3/5:\n","    Train Loss: 0.0816 | Train Acc: 0.9801\n","    Val Acc: 0.9205 | Val F1: 0.9203\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 4/5:\n","    Train Loss: 0.0527 | Train Acc: 0.9889\n","    Val Acc: 0.9195 | Val F1: 0.9190\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 5/5:\n","    Train Loss: 0.0217 | Train Acc: 0.9948\n","    Val Acc: 0.9061 | Val F1: 0.9056\n","\n","âœ… Trial 1 ì™„ë£Œ\n","   Best Val Accuracy: 0.9205 (92.05%)\n","   Best Val F1: 0.9203\n","\n","ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-11-10 17:59:06,913] Trial 0 finished with value: 0.9205366357069144 and parameters: {'batch_size': 16, 'epochs': 5, 'learning_rate': 2.620863021537753e-05, 'max_length': 256}. Best is trial 0 with value: 0.9205366357069144.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\n","  â€¢ í• ë‹¹ë¨: 4.80 GB\n","  â€¢ ì˜ˆì•½ë¨: 5.11 GB\n","\n","======================================================================\n","Trial 2\n","======================================================================\n","\n","ğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 16\n","  â€¢ Epochs: 4\n","  â€¢ Learning Rate: 3.13e-05\n","  â€¢ Max Length: 512\n","\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\n"]},{"output_type":"stream","name":"stderr","text":["                                                  "]},{"output_type":"stream","name":"stdout","text":["\n","âŒ Trial 2 ì‹¤íŒ¨: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [16, 512].  Tensor sizes: [1, 300]\n","ìƒì„¸ ì—ëŸ¬:\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-3656039876.py\", line 617, in objective\n","    outputs = model(\n","              ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 1482, in forward\n","    outputs = self.bert(\n","              ^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 931, in forward\n","    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n","                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","RuntimeError: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [16, 512].  Tensor sizes: [1, 300]\n","\n","\n","ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["\r[I 2025-11-10 17:59:08,250] Trial 1 finished with value: 0.0 and parameters: {'batch_size': 16, 'epochs': 4, 'learning_rate': 3.12551431816761e-05, 'max_length': 512}. Best is trial 0 with value: 0.9205366357069144.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n","\n","======================================================================\n","Trial 3\n","======================================================================\n","\n","ğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 8\n","  â€¢ Epochs: 3\n","  â€¢ Learning Rate: 1.34e-05\n","  â€¢ Max Length: 512\n","\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","âŒ Trial 3 ì‹¤íŒ¨: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 300]\n","ìƒì„¸ ì—ëŸ¬:\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-3656039876.py\", line 617, in objective\n","    outputs = model(\n","              ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 1482, in forward\n","    outputs = self.bert(\n","              ^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 931, in forward\n","    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n","                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","RuntimeError: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 300]\n","\n","\n","ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-11-10 17:59:09,723] Trial 2 finished with value: 0.0 and parameters: {'batch_size': 8, 'epochs': 3, 'learning_rate': 1.34336568680343e-05, 'max_length': 512}. Best is trial 0 with value: 0.9205366357069144.\n"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","Trial 4\n","======================================================================\n","\n","ğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 8\n","  â€¢ Epochs: 4\n","  â€¢ Learning Rate: 1.25e-05\n","  â€¢ Max Length: 512\n","\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","âŒ Trial 4 ì‹¤íŒ¨: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 300]\n","ìƒì„¸ ì—ëŸ¬:\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-3656039876.py\", line 617, in objective\n","    outputs = model(\n","              ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 1482, in forward\n","    outputs = self.bert(\n","              ^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\", line 931, in forward\n","    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n","                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","RuntimeError: The expanded size of the tensor (512) must match the existing size (300) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 300]\n","\n","\n","ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-11-10 17:59:10,816] Trial 3 finished with value: 0.0 and parameters: {'batch_size': 8, 'epochs': 4, 'learning_rate': 1.251705107614021e-05, 'max_length': 512}. Best is trial 0 with value: 0.9205366357069144.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n","\n","======================================================================\n","Trial 5\n","======================================================================\n","\n","ğŸ“Š ì œì•ˆëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 16\n","  â€¢ Epochs: 3\n","  â€¢ Learning Rate: 2.29e-05\n","  â€¢ Max Length: 256\n","\n","ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì‹œì‘):\n","  â€¢ í• ë‹¹ë¨: 3.99 GB\n","  â€¢ ì˜ˆì•½ë¨: 4.05 GB\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 1/3:\n","    Train Loss: 0.4647 | Train Acc: 0.8418\n","    Val Acc: 0.8937 | Val F1: 0.8922\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 2/3:\n","    Train Loss: 0.1741 | Train Acc: 0.9458\n","    Val Acc: 0.9102 | Val F1: 0.9095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["  Epoch 3/3:\n","    Train Loss: 0.0864 | Train Acc: 0.9765\n","    Val Acc: 0.9071 | Val F1: 0.9068\n","\n","âœ… Trial 5 ì™„ë£Œ\n","   Best Val Accuracy: 0.9102 (91.02%)\n","   Best Val F1: 0.9095\n","\n","ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-11-10 18:08:21,598] Trial 4 finished with value: 0.9102167182662538 and parameters: {'batch_size': 16, 'epochs': 3, 'learning_rate': 2.2878863522445918e-05, 'max_length': 256}. Best is trial 0 with value: 0.9205366357069144.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ’¾ GPU ë©”ëª¨ë¦¬ (Trial ì¢…ë£Œ):\n","  â€¢ í• ë‹¹ë¨: 4.80 GB\n","  â€¢ ì˜ˆì•½ë¨: 5.17 GB\n","\n","======================================================================\n","âœ… Optuna íŠœë‹ ì™„ë£Œ!\n","======================================================================\n","\n","======================================================================\n","14. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°\n","======================================================================\n","\n","ğŸ† ìµœê³  ì„±ëŠ¥ Trial: #0\n","   ê²€ì¦ ì •í™•ë„: 0.9205 (92.05%)\n","\n","ğŸ“Š ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ batch_size: 16\n","  â€¢ epochs: 5\n","  â€¢ learning_rate: 2.620863021537753e-05\n","  â€¢ max_length: 256\n","\n","======================================================================\n","15. ëª¨ë“  Trial ê²°ê³¼\n","======================================================================\n","\n","ì´ ì‹œë„ íšŸìˆ˜: 5\n","ì„±ê³µí•œ Trial: 5\n","Pruned Trial: 0\n","\n","ìµœìƒìœ„ 5ê°œ Trial:\n","   number     value  params_batch_size  params_epochs  params_learning_rate  params_max_length\n","0       0  0.920537                 16              5              0.000026                256\n","4       4  0.910217                 16              3              0.000023                256\n","1       1  0.000000                 16              4              0.000031                512\n","2       2  0.000000                  8              3              0.000013                512\n","3       3  0.000000                  8              4              0.000013                512\n","\n","======================================================================\n","16. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\n","======================================================================\n","\n","ğŸ“ í•™ìŠµ ì„¤ì •:\n","  â€¢ Batch Size: 16\n","  â€¢ Epochs: 5\n","  â€¢ Learning Rate: 2.62e-05\n","  â€¢ Max Length: 256\n","  â€¢ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: ì ìš© âœ…\n","\n","ğŸ”„ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [02:47<00:00,  1.45it/s, loss=0.4705, acc=0.8341]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Epoch 1/5 - Train Loss: 0.4705 | Train Acc: 0.8341 | Val Acc: 0.9123 | Val F1: 0.9121\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [02:48<00:00,  1.45it/s, loss=0.1486, acc=0.9567]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Epoch 2/5 - Train Loss: 0.1486 | Train Acc: 0.9567 | Val Acc: 0.9195 | Val F1: 0.9194\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [02:48<00:00,  1.45it/s, loss=0.0604, acc=0.9822]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Epoch 3/5 - Train Loss: 0.0604 | Train Acc: 0.9822 | Val Acc: 0.9164 | Val F1: 0.9155\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [02:48<00:00,  1.45it/s, loss=0.0397, acc=0.9897]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Epoch 4/5 - Train Loss: 0.0397 | Train Acc: 0.9897 | Val Acc: 0.9216 | Val F1: 0.9211\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [02:48<00:00,  1.45it/s, loss=0.0187, acc=0.9948]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Epoch 5/5 - Train Loss: 0.0187 | Train Acc: 0.9948 | Val Acc: 0.9112 | Val F1: 0.9101\n","\n","âœ“ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n","  ìµœê³  ê²€ì¦ ì •í™•ë„: 0.9216 (92.16%)\n","\n","======================================================================\n","17. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n","======================================================================\n","âœ“ ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°: 500ê°œ\n","âœ“ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ\n","\n","======================================================================\n","18. í…ŒìŠ¤íŠ¸ Dataset ìƒì„±\n","======================================================================\n","âœ“ í…ŒìŠ¤íŠ¸ DataLoader ìƒì„± ì™„ë£Œ\n","\n","======================================================================\n","19. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜ (ìµœì í™”ëœ ì‹ ë¢°ë„ ì„ê³„ê°’)\n","======================================================================\n","\n","ğŸ“Š ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ ì„¤ì • (ìµœì í™”ë¨):\n","  â€¢ High: â‰¥ 0.85 - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","  â€¢ Mid: 0.6 ~ 0.85 - ì˜ˆì¸¡ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","  â€¢ Low: < 0.6 - 'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜\n","  â€¢ Top-K ì˜ˆì¸¡ ê°œìˆ˜: 3\n","\n","ğŸ”„ ë¶„ë¥˜ ì¤‘...\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:07<00:00,  4.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ“ ë¶„ë¥˜ ì™„ë£Œ!\n","\n","======================================================================\n","ğŸ“Š ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ í†µê³„\n","======================================================================\n","\n","ì‹ ë¢°ë„ ë“±ê¸‰ë³„ ë¶„í¬:\n","  High:  447ê°œ ( 89.4%)\n","  Mid :   35ê°œ (  7.0%)\n","  Low :   18ê°œ (  3.6%)\n","\n","'ì¼ë°˜ ëŒ€í™”'ë¡œ ì¬ë¶„ë¥˜:\n","    18ê°œ (  3.6%)\n","\n","ì¬ë¶„ë¥˜ëœ í•­ëª©ì˜ ì›ë³¸ ì˜ˆì¸¡ ë¶„í¬:\n","  ê°ˆì·¨ ëŒ€í™”               :   6ê°œ\n","  í˜‘ë°• ëŒ€í™”               :   5ê°œ\n","  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”         :   5ê°œ\n","  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”           :   2ê°œ\n","\n","======================================================================\n","20. ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","======================================================================\n","âœ“ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ\n","\n","======================================================================\n","21. ê²°ê³¼ ì €ì¥ (Google Drive)\n","======================================================================\n","âœ“ ìƒì„¸ ë²„ì „ ì €ì¥: predictions_detailed.csv\n","âœ“ ê°„ë‹¨ ë²„ì „ ì €ì¥: predictions_simple.csv\n","âœ“ ì¬ë¶„ë¥˜ ë²„ì „ ì €ì¥: predictions_reclassified.csv (18ê°œ)\n","âœ“ ì‹ ë¢° ë²„ì „ ì €ì¥: predictions_trusted.csv (482ê°œ)\n","\n","âœ“ ì‹ ë¢°ë„ ë¶„ì„ ì‹œê°í™” ì €ì¥: confidence_analysis.png\n","\n","\n","\n","â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","â–ˆ                                                                    â–ˆ\n","â–ˆ                          âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!                         â–ˆ\n","â–ˆ                                                                    â–ˆ\n","â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","\n","ğŸ“ ì €ì¥ëœ íŒŒì¼ ì •ë³´:\n","\n","  ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ (4ê°€ì§€ ë²„ì „):\n","     1. predictions_detailed.csv - ì „ì²´ ìƒì„¸ (500ê°œ)\n","     2. predictions_simple.csv - ìµœì¢… ì˜ˆì¸¡ë§Œ (500ê°œ)\n","     3. predictions_reclassified.csv - 'ì¼ë°˜ ëŒ€í™”' ì¬ë¶„ë¥˜ (18ê°œ)\n","     4. predictions_trusted.csv - High+Mid ì‹ ë¢°ë„ (482ê°œ)\n","\n","  ğŸ“ˆ ì‹œê°í™”:\n","     5. confidence_analysis.png - ì‹ ë¢°ë„ ë¶„ì„\n","\n","ğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n","  â€¢ Batch Size: 16\n","  â€¢ Epochs: 5\n","  â€¢ Learning Rate: 2.62e-05\n","  â€¢ Max Length: 256\n","\n","ğŸ“ˆ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:\n","  â€¢ ê²€ì¦ ì •í™•ë„: 0.9216 (92.16%)\n","\n","ğŸ¯ ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜ í†µê³„:\n","  â€¢ High (â‰¥0.85): 447ê°œ (89.4%)\n","  â€¢ Mid (0.6~0.85): 35ê°œ (7.0%)\n","  â€¢ Low (<0.6): 18ê°œ (3.6%) â†’ 'ì¼ë°˜ ëŒ€í™”' ì¬ë¶„ë¥˜\n","\n","ğŸ”¥ ì ìš©ëœ ê°œì„ ì‚¬í•­:\n","  âœ… ì™„ë²½í•œ ì „ì²˜ë¦¬ (BOM, ê³µë°±, ë”°ì˜´í‘œ, ê¸¸ì´)\n","  âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© (ë¶ˆê· í˜• í•´ê²°)\n","  âœ… ì‹ ë¢°ë„ ì„ê³„ê°’ ìµœì í™” (0.8â†’0.6, 0.9â†’0.85)\n","  âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n","  âœ… í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ì²´í¬\n","\n","â±ï¸  ìˆ˜í–‰ ì‹œê°„:\n","  â€¢ Optuna íŠœë‹: 1472.6ì´ˆ (24.5ë¶„)\n","\n","ğŸ’¾ ëª¨ë“  íŒŒì¼ì´ Google Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n","\n","â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# ì•„ì›ƒí’‹ íŒŒì¼ì„ í†µí•œ ê²°ê³¼ ìš”ì•½\n","# ============================================================================\n","import pandas as pd\n","\n","df_pred = pd.read_csv('/content/drive/My Drive/predictions_detailed.csv')\n","\n","print(\"\\n\" + \"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½\".center(50, \"=\"))\n","print(f\"\\nì´ ì˜ˆì¸¡: {len(df_pred):,}ê°œ\\n\")\n","\n","print(\"ğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n","\n","if 'confidence' in df_pred.columns:\n","    print(f\"\\nğŸ“Œ ì „ì²´ ì‹ ë¢°ë„ ìš”ì•½:\")\n","    print(\"-\" * 50)\n","    print(f\"í‰ê· : {df_pred['confidence'].mean():.4f}\")\n","    print(f\"ìµœì†Œ: {df_pred['confidence'].min():.4f}\")\n","    print(f\"ìµœëŒ€: {df_pred['confidence'].max():.4f}\")\n","\n","    high = (df_pred['confidence'] >= 0.9).sum()\n","    mid = ((df_pred['confidence'] >= 0.7) & (df_pred['confidence'] < 0.9)).sum()\n","    low = (df_pred['confidence'] < 0.7).sum()\n","\n","    print(f\"\\në†’ì€ ì‹ ë¢°ë„ (â‰¥0.9):   {high:>5}ê°œ ({high/len(df_pred)*100:>5.1f}%)\")\n","    print(f\"ì¤‘ê°„ ì‹ ë¢°ë„ (0.7~0.9): {mid:>5}ê°œ ({mid/len(df_pred)*100:>5.1f}%)\")\n","    print(f\"ë‚®ì€ ì‹ ë¢°ë„ (<0.7):   {low:>5}ê°œ ({low/len(df_pred)*100:>5.1f}%)\")\n","\n","    # âœ¨ í´ë˜ìŠ¤ë³„ ìƒì„¸ ì‹ ë¢°ë„ ë¶„í¬\n","    print(f\"\\nğŸ“Œ í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„:\")\n","    print(\"=\" * 90)\n","\n","    # í‰ê·  ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n","    class_stats = []\n","    for class_name in df_pred['prediction'].unique():\n","        class_data = df_pred[df_pred['prediction'] == class_name]\n","        avg_conf = class_data['confidence'].mean()\n","        class_stats.append((class_name, avg_conf))\n","\n","    class_stats.sort(key=lambda x: x[1], reverse=True)\n","\n","    for class_name, _ in class_stats:\n","        class_data = df_pred[df_pred['prediction'] == class_name]\n","\n","        count = len(class_data)\n","        avg_conf = class_data['confidence'].mean()\n","        min_conf = class_data['confidence'].min()\n","        max_conf = class_data['confidence'].max()\n","        std_conf = class_data['confidence'].std()\n","\n","        high_class = (class_data['confidence'] >= 0.9).sum()\n","        mid_class = ((class_data['confidence'] >= 0.7) & (class_data['confidence'] < 0.9)).sum()\n","        low_class = (class_data['confidence'] < 0.7).sum()\n","\n","        high_pct = high_class / count * 100 if count > 0 else 0\n","        mid_pct = mid_class / count * 100 if count > 0 else 0\n","        low_pct = low_class / count * 100 if count > 0 else 0\n","\n","        print(f\"\\nğŸ”¹ {class_name} (ì´ {count}ê°œ)\")\n","        print(f\"   í‰ê· : {avg_conf:.4f} | ìµœì†Œ: {min_conf:.4f} | ìµœëŒ€: {max_conf:.4f} | í‘œì¤€í¸ì°¨: {std_conf:.4f}\")\n","        print(f\"   ë†’ìŒ (â‰¥0.9):   {high_class:>4}ê°œ ({high_pct:>5.1f}%)\", end=\"\")\n","        print(f\" {'â–ˆ' * int(high_pct / 5)}\")\n","        print(f\"   ì¤‘ê°„ (0.7~0.9): {mid_class:>4}ê°œ ({mid_pct:>5.1f}%)\", end=\"\")\n","        print(f\" {'â–ˆ' * int(mid_pct / 5)}\")\n","        print(f\"   ë‚®ìŒ (<0.7):   {low_class:>4}ê°œ ({low_pct:>5.1f}%)\", end=\"\")\n","        print(f\" {'â–ˆ' * int(low_pct / 5)}\")\n","\n","    print(\"\\n\" + \"=\" * 90)\n","\n","print(\"\\n\" + \"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyGtiPne12yq","executionInfo":{"status":"ok","timestamp":1762799151600,"user_tz":-540,"elapsed":113,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"}},"outputId":"397996b2-888b-4622-de53-7eab4661cf83"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","====================ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½====================\n","\n","ì´ ì˜ˆì¸¡: 500ê°œ\n","\n","ğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬:\n","\n","ğŸ“Œ ì „ì²´ ì‹ ë¢°ë„ ìš”ì•½:\n","--------------------------------------------------\n","í‰ê· : 0.9502\n","ìµœì†Œ: 0.3171\n","ìµœëŒ€: 0.9994\n","\n","ë†’ì€ ì‹ ë¢°ë„ (â‰¥0.9):     436ê°œ ( 87.2%)\n","ì¤‘ê°„ ì‹ ë¢°ë„ (0.7~0.9):    30ê°œ (  6.0%)\n","ë‚®ì€ ì‹ ë¢°ë„ (<0.7):      34ê°œ (  6.8%)\n","\n","ğŸ“Œ í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ìƒì„¸ ë¶„ì„:\n","==========================================================================================\n","\n","ğŸ”¹ ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™” (ì´ 140ê°œ)\n","   í‰ê· : 0.9774 | ìµœì†Œ: 0.6149 | ìµœëŒ€: 0.9994 | í‘œì¤€í¸ì°¨: 0.0653\n","   ë†’ìŒ (â‰¥0.9):    129ê°œ ( 92.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","   ì¤‘ê°„ (0.7~0.9):    7ê°œ (  5.0%) â–ˆ\n","   ë‚®ìŒ (<0.7):      4ê°œ (  2.9%) \n","\n","ğŸ”¹ í˜‘ë°• ëŒ€í™” (ì´ 90ê°œ)\n","   í‰ê· : 0.9761 | ìµœì†Œ: 0.6045 | ìµœëŒ€: 0.9993 | í‘œì¤€í¸ì°¨: 0.0709\n","   ë†’ìŒ (â‰¥0.9):     83ê°œ ( 92.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","   ì¤‘ê°„ (0.7~0.9):    4ê°œ (  4.4%) \n","   ë‚®ìŒ (<0.7):      3ê°œ (  3.3%) \n","\n","ğŸ”¹ ê°ˆì·¨ ëŒ€í™” (ì´ 120ê°œ)\n","   í‰ê· : 0.9731 | ìµœì†Œ: 0.6047 | ìµœëŒ€: 0.9990 | í‘œì¤€í¸ì°¨: 0.0706\n","   ë†’ìŒ (â‰¥0.9):    112ê°œ ( 93.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","   ì¤‘ê°„ (0.7~0.9):    4ê°œ (  3.3%) \n","   ë‚®ìŒ (<0.7):      4ê°œ (  3.3%) \n","\n","ğŸ”¹ ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™” (ì´ 130ê°œ)\n","   í‰ê· : 0.9498 | ìµœì†Œ: 0.6052 | ìµœëŒ€: 0.9992 | í‘œì¤€í¸ì°¨: 0.0885\n","   ë†’ìŒ (â‰¥0.9):    110ê°œ ( 84.6%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","   ì¤‘ê°„ (0.7~0.9):   15ê°œ ( 11.5%) â–ˆâ–ˆ\n","   ë‚®ìŒ (<0.7):      5ê°œ (  3.8%) \n","\n","ğŸ”¹ ì¼ë°˜ ëŒ€í™” (ì´ 20ê°œ)\n","   í‰ê· : 0.5085 | ìµœì†Œ: 0.3171 | ìµœëŒ€: 0.9960 | í‘œì¤€í¸ì°¨: 0.1722\n","   ë†’ìŒ (â‰¥0.9):      2ê°œ ( 10.0%) â–ˆâ–ˆ\n","   ì¤‘ê°„ (0.7~0.9):    0ê°œ (  0.0%) \n","   ë‚®ìŒ (<0.7):     18ê°œ ( 90.0%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n","\n","==========================================================================================\n","\n","======================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"HOIKRGYCBm-5"},"source":["ìš”ì•½ ë° ê°œì„ ì•ˆ\n","1) ì–´ì œ ìµœê³  í‰ì ì´ ë‚˜ì™”ë˜ ê°œì„ ì•ˆì´ í† ìš”ì¼ ìƒˆë²½ ë²„ì „ì˜ ì•„ì›ƒí’‹ì¸ë° ì´í›„ ê³„ì† ìˆ˜ì •ë˜ê³  ì™”ê¸° ë•Œë¬¸ì—, í˜„ì¬ ì–´ë–¤ ìƒíƒœì—ì„œ ê·¸ëŸ° ê²°ê³¼ë¬¼ì´ ë‚˜ì™”ëŠ”ì§€ ê·¸ëŒ€ë¡œ ì¬í˜„ë˜ê³  ìˆì§€ ì•ŠìŒ (ì•ˆíƒ€ê¹ìŠµë‹ˆë‹¤.) ê·¸ëŸ¬ë‚˜, ê°œì„ ë°©í–¥ê³¼ í¬í…ì…œì€ ëª…í™•í•˜ê²Œ ë³´ì„\n","1) ì¼ë°˜ ëŒ€í™”ì˜ ë‹¤ì–‘ì„± ë¶€ì—¬ ë° ê°•í™”. ì´ë¥¼ ìœ„í•´ ì¼ë°˜ ëŒ€í™” ìƒì„±ê¸°ë¥¼ ë§Œë“¤ì–´ ë³¼ ìƒê°ì„. ê·¸ë˜ì•¼ ì—¬ëŸ¬ê°€ì§€ ë¡œì§ì„ ë°˜ì˜í•œ ì¼ë°˜ ëŒ€í™”ë¥¼ ìƒì„±í•˜ê³  í†µí•©í•´ì•¼ í’ë¶€í•œ ì •ë³´ë¥¼ ë‹´ì€ ì¼ë°˜ ëŒ€í™” í•©ì„±ì´ ê°€ëŠ¥ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë¨\n","2) ë¶„ë¥˜ì‹œ í™•ë¥ ì— ë”°ë¼ ì‹¤í–‰í•˜ëŠ”ë°, ì´ë•Œ í™•ë¥ ì— ë”°ë¥¸ ì»¨í”¼ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ë¶„ë¥˜ í•¨. ë¶ˆê· í˜•ì ì¸ ë¶„ë¥˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬ ë‚®ì€ ì»¨í”¼ë˜ìŠ¤ë¥¼ ê°€ì§„ ê²½ìš° ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ ì ìš© (í˜¹ì€  ì„ê³„ê°’ ë°©ì‹ ë°˜ì˜)í•´ ë³´ê³ ì í•¨. ì¶”ê°€ ì½”ë“œ ë¸”ëŸ­ì— ì˜ˆì‹œë¥¼ ì œì‹œí•¨"]},{"cell_type":"markdown","source":["# **ì¶”ê°€ ë¸”ëŸ­**"],"metadata":{"id":"dgAbcTod51oO"}},{"cell_type":"code","source":["#ì œì¶œí¬ë§· ë³€ê²½ - path ì˜ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.\n","# 1. CSV íŒŒì¼ ì½ê¸°\n","df = pd.read_csv('/content/drive/My Drive/predictions_detailed.csv') #íŒ¨ìŠ¤ ì§€ì •!\n","\n","print(\"ì›ë³¸ íŒŒì¼ í™•ì¸:\")\n","print(f\"í–‰ ìˆ˜: {len(df)}\")\n","print(f\"ì—´: {df.columns.tolist()}\")\n","print(\"\\nì²˜ìŒ 5í–‰:\")\n","print(df.head())\n","\n","# 2. idxì™€ prediction ì—´ë§Œ ì„ íƒ\n","df_selected = df[['idx', 'prediction']].copy()\n","\n","# 3. prediction ì—´ì„ classë¡œ ì´ë¦„ ë³€ê²½\n","df_selected.rename(columns={'prediction': 'class'}, inplace=True)\n","\n","# 4. ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n","class_mapping = {\n","    'í˜‘ë°• ëŒ€í™”': 0,\n","    'ê°ˆì·¨ ëŒ€í™”': 1,\n","    'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': 2,\n","    'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': 3,\n","    'ì¼ë°˜ ëŒ€í™”': 4\n","}\n","\n","# 5. prediction ê°’ì„ ìˆ«ìë¡œ ë³€í™˜\n","# Note: The column is now named 'class', so we apply the mapping to that column.\n","df_selected['class'] = df_selected['class'].map(class_mapping)\n","\n","\n","# 6. ê²°ê³¼ í™•ì¸\n","print(\"\\n\\në³€í™˜ í›„ íŒŒì¼ í™•ì¸:\")\n","print(f\"í–‰ ìˆ˜: {len(df_selected)}\")\n","print(f\"ì—´: {df_selected.columns.tolist()}\")\n","print(f\"ë°ì´í„° ëª¨ì–‘: {df_selected.shape}\") # Add this line to show the shape\n","print(\"\\nì²˜ìŒ 10è¡Œ:\")\n","print(df_selected.head(10))\n","\n","# 7. ë³€í™˜ëœ ê°’ í™•ì¸\n","print(\"\\n\\në³€í™˜ í†µê³„:\")\n","print(df_selected['class'].value_counts().sort_index())\n","\n","# 8. CSVë¡œ ì €ì¥ (ì˜ˆ: êµ¬ê¸€ ë“œë¼ì´ë¸Œ My Drive ë£¨íŠ¸ì— ì €ì¥)\n","output_path = '/content/drive/My Drive/prediction_final.csv'  # ì €ì¥í•  ê²½ë¡œ ì§€ì •\n","df_selected.to_csv(output_path, index=False, encoding='utf-8-sig')\n","\n","print(f\"\\në³€í™˜ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n"],"metadata":{"id":"2QVghl8h3eSo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762799511985,"user_tz":-540,"elapsed":79,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"}},"outputId":"43e576d8-3ddb-4d59-e76c-a7a9e024427c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["ì›ë³¸ íŒŒì¼ í™•ì¸:\n","í–‰ ìˆ˜: 500\n","ì—´: ['idx', 'conversation', 'prediction', 'original_prediction', 'confidence', 'confidence_level', 'reclassified_to_normal', 'top_k_predictions', 'pred_rank_1', 'conf_rank_1', 'pred_rank_2', 'conf_rank_2', 'pred_rank_3', 'conf_rank_3']\n","\n","ì²˜ìŒ 5í–‰:\n","     idx                                       conversation   prediction  \\\n","0  t_000  ì•„ê°€ì”¨ ë‹´ë°°í•œê°‘ì£¼ì†Œ ë„¤ 4500ì›ì…ë‹ˆë‹¤ ì–´ ë„¤ ì§€ê°‘ì–´ë””ê°”ì§€ ì—ì´ ë²„ìŠ¤ì—ì„œ ìƒì–´ë²„ë ¸ë‚˜...        ê°ˆì·¨ ëŒ€í™”   \n","1  t_001  ìš°ë¦¬íŒ€ì—ì„œ ë‹¤ë¥¸íŒ€ìœ¼ë¡œ ê°ˆ ì‚¬ëŒ ì—†ë‚˜? ê·¸ëŸ¼ ì˜ì§€ì”¨ê°€ ê°€ëŠ”ê±´ ì–´ë•Œ? ë„¤? ì œê°€ìš”? ê·¸...  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”   \n","2  t_002  ë„ˆ ì˜¤ëŠ˜ ê·¸ê²Œ ë­ì•¼ ë„¤ ì œê°€ ë­˜ ì˜ëª»í–ˆë‚˜ìš”.? ì œëŒ€ë¡œ ì¢€ í•˜ì§€ ë„¤ ë˜‘ë°”ë¡œ ì¢€ í•˜ì§€ ...  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”   \n","3  t_003  ì´ê±° ë“¤ì–´ë°” ì™€ ì´ ë…¸ë˜ ì§„ì§œ ì¢‹ë‹¤ ê·¸ì¹˜ ìš”ì¦˜ ì´ ê²ƒë§Œ ë“¤ì–´ ì§„ì§œ ë„ˆë¬´ ì¢‹ë‹¤ ë‚´ê°€ ...    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”   \n","4  t_004  ì•„ë¬´íŠ¼ ì•ìœ¼ë¡œ ë‹ˆê°€ ë‚´ ì™€ì´íŒŒì´ì•¼. .ì‘ ì™€ì´íŒŒì´ ì˜¨. ì¼°ì–´. ë°˜ë§? ì£¼ì¸ë‹˜ì´ë¼ê³ ë„...        ì¼ë°˜ ëŒ€í™”   \n","\n","  original_prediction  confidence confidence_level  reclassified_to_normal  \\\n","0               ê°ˆì·¨ ëŒ€í™”    0.998091             High                   False   \n","1         ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”    0.999237             High                   False   \n","2         ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”    0.843358              Mid                   False   \n","3           ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”    0.983577             High                   False   \n","4               í˜‘ë°• ëŒ€í™”    0.383660              Low                    True   \n","\n","                                   top_k_predictions  pred_rank_1  \\\n","0     ê°ˆì·¨ ëŒ€í™”(0.998) | ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”(0.001) | í˜‘ë°• ëŒ€í™”(0.001)        ê°ˆì·¨ ëŒ€í™”   \n","1   ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”(0.999) | ì¼ë°˜ ëŒ€í™”(0.000) | í˜‘ë°• ëŒ€í™”(0.000)  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”   \n","2  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”(0.843) | í˜‘ë°• ëŒ€í™”(0.151) | ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”(...  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”   \n","3  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”(0.984) | ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”(0.008) | ì¼ë°˜ ëŒ€í™”(...    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”   \n","4     í˜‘ë°• ëŒ€í™”(0.384) | ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”(0.382) | ê°ˆì·¨ ëŒ€í™”(0.182)        í˜‘ë°• ëŒ€í™”   \n","\n","   conf_rank_1  pred_rank_2  conf_rank_2 pred_rank_3  conf_rank_3  \n","0       0.9981    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.0008       í˜‘ë°• ëŒ€í™”       0.0006  \n","1       0.9992        ì¼ë°˜ ëŒ€í™”       0.0003       í˜‘ë°• ëŒ€í™”       0.0003  \n","2       0.8434        í˜‘ë°• ëŒ€í™”       0.1506   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.0036  \n","3       0.9836  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”       0.0077       ì¼ë°˜ ëŒ€í™”       0.0072  \n","4       0.3837    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”       0.3824       ê°ˆì·¨ ëŒ€í™”       0.1824  \n","\n","\n","ë³€í™˜ í›„ íŒŒì¼ í™•ì¸:\n","í–‰ ìˆ˜: 500\n","ì—´: ['idx', 'class']\n","ë°ì´í„° ëª¨ì–‘: (500, 2)\n","\n","ì²˜ìŒ 10è¡Œ:\n","     idx  class\n","0  t_000      1\n","1  t_001      2\n","2  t_002      2\n","3  t_003      3\n","4  t_004      4\n","5  t_005      0\n","6  t_006      0\n","7  t_007      1\n","8  t_008      2\n","9  t_009      1\n","\n","\n","ë³€í™˜ í†µê³„:\n","class\n","0     90\n","1    120\n","2    140\n","3    130\n","4     20\n","Name: count, dtype: int64\n","\n","ë³€í™˜ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: /content/drive/My Drive/prediction_final.csv\n"]}]},{"cell_type":"code","source":["# ë°ì´í„° ì„ë² ë”© êµ¬ì¡° ë° í…ŒìŠ¤íŠ¸, í…ŒìŠ¤íŠ¸ ë¸”ëŸ­\n","# ============================================================================\n","# ğŸ” ëª¨ë¸ êµ¬ì¡° ì§„ë‹¨\n","# ============================================================================\n","print(\"=\"*70)\n","print(\"ëª¨ë¸ êµ¬ì¡° ì§„ë‹¨\")\n","print(\"=\"*70)\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification\n","\n","# 1. Config í™•ì¸\n","config = AutoConfig.from_pretrained('beomi/kcbert-base')\n","print(f\"\\nğŸ“‹ ëª¨ë¸ Config:\")\n","print(f\"  â€¢ max_position_embeddings: {config.max_position_embeddings}\")\n","print(f\"  â€¢ hidden_size: {config.hidden_size}\")\n","print(f\"  â€¢ vocab_size: {config.vocab_size}\")\n","\n","# 2. í† í¬ë‚˜ì´ì € í™•ì¸\n","tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n","print(f\"\\nğŸ”¤ í† í¬ë‚˜ì´ì €:\")\n","print(f\"  â€¢ model_max_length: {tokenizer.model_max_length}\")\n","print(f\"  â€¢ vocab_size: {len(tokenizer)}\")\n","\n","# 3. ì‹¤ì œ ëª¨ë¸ ë¡œë“œ í›„ ì„ë² ë”© í¬ê¸° í™•ì¸\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    'beomi/kcbert-base',\n","    num_labels=len(label_encoder.classes_)\n",")\n","\n","print(f\"\\nğŸ§  ëª¨ë¸ ë ˆì´ì–´:\")\n","print(f\"  â€¢ Position Embeddings: {model.bert.embeddings.position_embeddings.weight.shape}\")\n","print(f\"  â€¢ Token Embeddings: {model.bert.embeddings.word_embeddings.weight.shape}\")\n","print(f\"  â€¢ Classifier: {model.classifier.weight.shape}\")\n","\n","# 4. í…ŒìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ í™•ì¸\n","test_text = \"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\"\n","# Use the smaller of tokenizer's model_max_length and config's max_position_embeddings\n","max_seq_length = min(tokenizer.model_max_length, config.max_position_embeddings)\n","\n","test_encoding = tokenizer(test_text, max_length=max_seq_length, padding='max_length',\n","                          truncation=True, return_tensors='pt')\n","print(f\"\\nğŸ§ª í…ŒìŠ¤íŠ¸ ì…ë ¥:\")\n","print(f\"  â€¢ input_ids shape: {test_encoding['input_ids'].shape}\")\n","print(f\"  â€¢ attention_mask shape: {test_encoding['attention_mask'].shape}\")\n","\n","try:\n","    with torch.no_grad():\n","        output = model(**test_encoding)\n","    print(f\"  â€¢ ëª¨ë¸ ì¶œë ¥: âœ… ì •ìƒ\")\n","except Exception as e:\n","    print(f\"  â€¢ ëª¨ë¸ ì¶œë ¥: âŒ ì—ëŸ¬ - {str(e)}\")\n","\n","print(\"=\"*70 + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdmuQA5oe1l1","executionInfo":{"status":"ok","timestamp":1762663167188,"user_tz":-540,"elapsed":2855,"user":{"displayName":"ì´ì˜ì„","userId":"00277657695281828763"}},"outputId":"74edd5a3-63ae-43f4-e1f3-d01635594f42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ëª¨ë¸ êµ¬ì¡° ì§„ë‹¨\n","======================================================================\n","\n","ğŸ“‹ ëª¨ë¸ Config:\n","  â€¢ max_position_embeddings: 300\n","  â€¢ hidden_size: 768\n","  â€¢ vocab_size: 30000\n","\n","ğŸ”¤ í† í¬ë‚˜ì´ì €:\n","  â€¢ model_max_length: 300\n","  â€¢ vocab_size: 30000\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ§  ëª¨ë¸ ë ˆì´ì–´:\n","  â€¢ Position Embeddings: torch.Size([300, 768])\n","  â€¢ Token Embeddings: torch.Size([30000, 768])\n","  â€¢ Classifier: torch.Size([5, 768])\n","\n","ğŸ§ª í…ŒìŠ¤íŠ¸ ì…ë ¥:\n","  â€¢ input_ids shape: torch.Size([1, 300])\n","  â€¢ attention_mask shape: torch.Size([1, 300])\n","  â€¢ ëª¨ë¸ ì¶œë ¥: âœ… ì •ìƒ\n","======================================================================\n","\n"]}]},{"cell_type":"code","source":["# ì»¨í”¼ë˜ìŠ¤ ê¸°ë°˜ ë¶„ë¥˜\n","# ë‚®ì€ confidenceë¥¼ ê°€ì§„ ê°ˆì·¨ â†’ ì¼ë°˜ìœ¼ë¡œ ì¬ë¶„ë¥˜\n","def reclassify_low_confidence(df, confidence_threshold=0.7):\n","    \"\"\"\n","    ê°ˆì·¨ë¡œ ë¶„ë¥˜ëì§€ë§Œ í™•ì‹ ì´ ë‚®ìœ¼ë©´ ì¼ë°˜ìœ¼ë¡œ ë³€ê²½\n","    \"\"\"\n","    df_copy = df.copy()\n","\n","    # ê°ˆì·¨ì¸ë° confidenceê°€ ë‚®ì€ ê²½ìš°\n","    mask = (df_copy['prediction'] == 'ê°ˆì·¨ ëŒ€í™”') & (df_copy['confidence'] < confidence_threshold)\n","    df_copy.loc[mask, 'prediction'] = 'ì¼ë°˜ ëŒ€í™”'\n","\n","    print(f\"ì¬ë¶„ë¥˜ëœ ìƒ˜í”Œ ìˆ˜: {mask.sum()}ê°œ\")\n","    return df_copy\n","\n","# ì‚¬ìš©\n","df_improved = reclassify_low_confidence(df, confidence_threshold=0.75)"],"metadata":{"id":"lZtYwY_BOUjz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXhs2x8fBqPr"},"outputs":[],"source":["# ì„ê³„ê°’ ë¶€ì—¬ ë°©ì‹ ë¶„ë¥˜\n","# í˜„ì¬: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ë¥¼ ì„ íƒ\n","predicted_class = np.argmax(predictions)\n","\n","# ê°œì„ : ì„ê³„ê°’ ì¡°ì •\n","def predict_with_threshold(predictions, thresholds):\n","    \"\"\"\n","    thresholds: {0: 0.6, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.3}\n","    ê°ˆì·¨(1)ëŠ” ë†’ì€ í™•ë¥  í•„ìš”, ì¼ë°˜(4)ì€ ë‚®ì€ í™•ë¥ ë¡œë„ í—ˆìš©\n","    \"\"\"\n","    max_class = np.argmax(predictions)\n","    max_prob = predictions[max_class]\n","\n","    # ì¼ë°˜ ëŒ€í™” ìš°ì„  ê²€í† \n","    if predictions[4] > thresholds[4]:  # ì¼ë°˜ì´ 30% ì´ìƒì´ë©´\n","        return 4\n","\n","    # ê°ˆì·¨ëŠ” ì—„ê²©í•˜ê²Œ\n","    if max_class == 1 and max_prob < thresholds[1]:  # ê°ˆì·¨ê°€ 50% ë¯¸ë§Œì´ë©´\n","        return 4  # ì¼ë°˜ìœ¼ë¡œ ë¶„ë¥˜\n","\n","    return max_class\n","\n","# ì‚¬ìš© ì˜ˆì‹œ\n","thresholds = {\n","    0: 0.6,  # í˜‘ë°•\n","    1: 0.7,  # ê°ˆì·¨ (ì—„ê²©í•˜ê²Œ!)\n","    2: 0.6,  # ì§ì¥ ë‚´ ê´´ë¡­í˜\n","    3: 0.6,  # ê¸°íƒ€ ê´´ë¡­í˜\n","    4: 0.2   # ì¼ë°˜ (ê´€ëŒ€í•˜ê²Œ!)\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1_euC_7y5brwyUsSEuPXLojgbCzCujXKO","timestamp":1762796226302},{"file_id":"1O5tz5pgeEsQaFPJpKuG_CtCJW2Ql5v3-","timestamp":1762651078073},{"file_id":"1i63OGIxEXQq5imkntKApMeWGec-gMa6R","timestamp":1762605497063},{"file_id":"1QuBMtmaSNKEEpyFwoynPab9susn_ko6r","timestamp":1762528334449},{"file_id":"1TKXYY1zpUB-5bKlfnhVwLCTNfCP-Kv23","timestamp":1762498512059}],"authorship_tag":"ABX9TyOJB7CxR50zcAEL0+TY5VBr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}